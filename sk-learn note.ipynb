{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"【 更新 】 柯 P ： 洪智坤 洩漏 公文 案還 沒 看到 公文 　 今處理\", \"留 洪智坤   柯 ： 殘障 求職 不易\", \"人事 處議 處 洪智坤 　 柯 P ： 不 清楚 議處 結果\"]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import json\n",
    "ary = ['【更新】柯P：洪智坤洩漏公文案還沒看到公文　今處理',\n",
    "'留洪智坤 柯：殘障求職不易',\n",
    "'人事處議處洪智坤　柯P：不清楚議處結果']\n",
    "corpus = []\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))\n",
    "\n",
    "jieba.add_word('洪智坤',100, 'nr')\n",
    "print (json.dumps(corpus,ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 4)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 12)\t1\n",
      "[array([u'\\u66f4\\u65b0', u'\\u6d2a\\u667a\\u5764', u'\\u6d29\\u6f0f',\n",
      "       u'\\u516c\\u6587', u'\\u6848\\u9084', u'\\u770b\\u5230',\n",
      "       u'\\u4eca\\u8655\\u7406'], \n",
      "      dtype='<U3'), array([u'\\u6d2a\\u667a\\u5764', u'\\u6b98\\u969c', u'\\u6c42\\u8077',\n",
      "       u'\\u4e0d\\u6613'], \n",
      "      dtype='<U3'), array([u'\\u6d2a\\u667a\\u5764', u'\\u4eba\\u4e8b', u'\\u8655\\u8b70',\n",
      "       u'\\u6e05\\u695a', u'\\u8b70\\u8655', u'\\u7d50\\u679c'], \n",
      "      dtype='<U3')]\n",
      "[\"不易\", \"人事\", \"今處理\", \"公文\", \"更新\", \"案還\", \"殘障\", \"求職\", \"洩漏\", \"洪智坤\", \"清楚\", \"看到\", \"結果\", \"處議\", \"議處\"]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print (type(X))\n",
    "print (X)\n",
    "print (vectorizer.inverse_transform(X))  #反轉transform列出原本文本的樣子\n",
    "print (json.dumps(vectorizer.get_feature_names(),ensure_ascii=False))   #列出文本所有feature原本的樣子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 2)\t0.327055479232\n",
      "  (0, 11)\t0.327055479232\n",
      "  (0, 5)\t0.327055479232\n",
      "  (0, 3)\t0.654110958465\n",
      "  (0, 8)\t0.327055479232\n",
      "  (0, 9)\t0.19316423462\n",
      "  (0, 4)\t0.327055479232\n",
      "  (1, 0)\t0.546454011634\n",
      "  (1, 7)\t0.546454011634\n",
      "  (1, 6)\t0.546454011634\n",
      "  (1, 9)\t0.32274454218\n",
      "  (2, 12)\t0.43238508879\n",
      "  (2, 14)\t0.43238508879\n",
      "  (2, 10)\t0.43238508879\n",
      "  (2, 13)\t0.43238508879\n",
      "  (2, 1)\t0.43238508879\n",
      "  (2, 9)\t0.255373598795\n",
      "[array([u'\\u4eca\\u8655\\u7406', u'\\u770b\\u5230', u'\\u6848\\u9084',\n",
      "       u'\\u516c\\u6587', u'\\u6d29\\u6f0f', u'\\u6d2a\\u667a\\u5764',\n",
      "       u'\\u66f4\\u65b0'], \n",
      "      dtype='<U3'), array([u'\\u4e0d\\u6613', u'\\u6c42\\u8077', u'\\u6b98\\u969c',\n",
      "       u'\\u6d2a\\u667a\\u5764'], \n",
      "      dtype='<U3'), array([u'\\u7d50\\u679c', u'\\u8b70\\u8655', u'\\u6e05\\u695a', u'\\u8655\\u8b70',\n",
      "       u'\\u4eba\\u4e8b', u'\\u6d2a\\u667a\\u5764'], \n",
      "      dtype='<U3')]\n",
      "[\"不易\", \"人事\", \"今處理\", \"公文\", \"更新\", \"案還\", \"殘障\", \"求職\", \"洩漏\", \"洪智坤\", \"清楚\", \"看到\", \"結果\", \"處議\", \"議處\"]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() \n",
    "Y = vectorizer.fit_transform(corpus)\n",
    "print (type(Y))\n",
    "print (Y)\n",
    "print (vectorizer.inverse_transform(Y))  #反轉transform列出原本文本的樣子\n",
    "print (json.dumps(vectorizer.get_feature_names(),ensure_ascii=False))   #列出文本所有feature原本的樣子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 246441)\t0.316227766017\n",
      "  (0, 492803)\t-0.316227766017\n",
      "  (0, 547271)\t0.316227766017\n",
      "  (0, 664113)\t-0.316227766017\n",
      "  (0, 717058)\t0.316227766017\n",
      "  (0, 982272)\t-0.632455532034\n",
      "  (0, 1034416)\t0.316227766017\n",
      "  (1, 96768)\t-0.5\n",
      "  (1, 104916)\t-0.5\n",
      "  (1, 246441)\t0.5\n",
      "  (1, 576637)\t0.5\n",
      "  (2, 16162)\t0.408248290464\n",
      "  (2, 246441)\t0.408248290464\n",
      "  (2, 377913)\t-0.408248290464\n",
      "  (2, 410670)\t-0.408248290464\n",
      "  (2, 463057)\t-0.408248290464\n",
      "  (2, 569169)\t-0.408248290464\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HashingVectorizer' object has no attribute 'inverse_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-84d162d989e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#error! hash不能反轉!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#error! hash不能反轉!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HashingVectorizer' object has no attribute 'inverse_transform'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer() \n",
    "Z = vectorizer.fit_transform(corpus)\n",
    "print (type(Z))\n",
    "print (Z)\n",
    "print (vectorizer.inverse_transform(Z))  #error! hash不能反轉!\n",
    "print (vectorizer.get_feature_names())   #error! hash不能反轉!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer/TfidfVectorizer/HashingVectorizer/HashingVectorizer+TfidfTransformer\n",
    "- 四轉將文本轉換為向量的比較，output為: (文本編號, feature編號)     分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== CountVectorizer ===========\n",
      "  (0, 4)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 12)\t1\n",
      "=========== TfidfVectorizer ===========\n",
      "  (0, 2)\t0.327055479232\n",
      "  (0, 11)\t0.327055479232\n",
      "  (0, 5)\t0.327055479232\n",
      "  (0, 3)\t0.654110958465\n",
      "  (0, 8)\t0.327055479232\n",
      "  (0, 9)\t0.19316423462\n",
      "  (0, 4)\t0.327055479232\n",
      "  (1, 0)\t0.546454011634\n",
      "  (1, 7)\t0.546454011634\n",
      "  (1, 6)\t0.546454011634\n",
      "  (1, 9)\t0.32274454218\n",
      "  (2, 12)\t0.43238508879\n",
      "  (2, 14)\t0.43238508879\n",
      "  (2, 10)\t0.43238508879\n",
      "  (2, 13)\t0.43238508879\n",
      "  (2, 1)\t0.43238508879\n",
      "  (2, 9)\t0.255373598795\n",
      "=========== HashingVectorizer ===========\n",
      "  (0, 246441)\t0.316227766017\n",
      "  (0, 492803)\t-0.316227766017\n",
      "  (0, 547271)\t0.316227766017\n",
      "  (0, 664113)\t-0.316227766017\n",
      "  (0, 717058)\t0.316227766017\n",
      "  (0, 982272)\t-0.632455532034\n",
      "  (0, 1034416)\t0.316227766017\n",
      "  (1, 96768)\t-0.5\n",
      "  (1, 104916)\t-0.5\n",
      "  (1, 246441)\t0.5\n",
      "  (1, 576637)\t0.5\n",
      "  (2, 16162)\t0.408248290464\n",
      "  (2, 246441)\t0.408248290464\n",
      "  (2, 377913)\t-0.408248290464\n",
      "  (2, 410670)\t-0.408248290464\n",
      "  (2, 463057)\t-0.408248290464\n",
      "  (2, 569169)\t-0.408248290464\n",
      "=========== HashingVectorizer+TfidfTransformer ===========\n",
      "  (0, 1034416)\t0.327055479232\n",
      "  (0, 982272)\t-0.654110958465\n",
      "  (0, 717058)\t0.327055479232\n",
      "  (0, 664113)\t-0.327055479232\n",
      "  (0, 547271)\t0.327055479232\n",
      "  (0, 492803)\t-0.327055479232\n",
      "  (0, 246441)\t0.19316423462\n",
      "  (1, 576637)\t0.546454011634\n",
      "  (1, 246441)\t0.32274454218\n",
      "  (1, 104916)\t-0.546454011634\n",
      "  (1, 96768)\t-0.546454011634\n",
      "  (2, 569169)\t-0.43238508879\n",
      "  (2, 463057)\t-0.43238508879\n",
      "  (2, 410670)\t-0.43238508879\n",
      "  (2, 377913)\t-0.43238508879\n",
      "  (2, 246441)\t0.255373598795\n",
      "  (2, 16162)\t0.43238508879\n"
     ]
    }
   ],
   "source": [
    "print(\"=========== CountVectorizer ===========\")\n",
    "print (X)\n",
    "print(\"=========== TfidfVectorizer ===========\")\n",
    "print (Y)\n",
    "print(\"=========== HashingVectorizer ===========\")\n",
    "print (Z)\n",
    "print(\"=========== HashingVectorizer+TfidfTransformer ===========\")\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(Z)\n",
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer/TfidfVectorizer/HashingVectorizer/HashingVectorizer+TfidfTransformer\n",
    "- toarray(): 此例為三個文本，output為三個文本各自佔有feature的分數矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== CountVectorizer ===========\n",
      "[[0 0 1 2 1 1 0 0 1 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 1 1 1]]\n",
      "=========== TfidfVectorizer ===========\n",
      "[[ 0.          0.          0.32705548  0.65411096  0.32705548  0.32705548\n",
      "   0.          0.          0.32705548  0.19316423  0.          0.32705548\n",
      "   0.          0.          0.        ]\n",
      " [ 0.54645401  0.          0.          0.          0.          0.\n",
      "   0.54645401  0.54645401  0.          0.32274454  0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.43238509  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.2553736   0.43238509  0.          0.43238509\n",
      "   0.43238509  0.43238509]]\n",
      "=========== HashingVectorizer ===========\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "=========== HashingVectorizer+TfidfTransformer ===========\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "array type:  <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"=========== CountVectorizer ===========\")\n",
    "print (X.toarray())\n",
    "print(\"=========== TfidfVectorizer ===========\")\n",
    "print (Y.toarray())\n",
    "print(\"=========== HashingVectorizer ===========\")\n",
    "print (Z.toarray())\n",
    "print(\"=========== HashingVectorizer+TfidfTransformer ===========\")\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(Z)\n",
    "print (tfidf.toarray())\n",
    "print \"array type: \",type(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_feature_names( ):CountVectorizer/TfidfVectorizer\n",
    "- 兩者因為算法不同，所以順序會不太一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== CountVectorizer ===========\n",
      "[\"更新\", \"洪智坤\", \"洩漏\", \"公文\", \"案還\", \"看到\", \"今處理\"]\n",
      "[\"洪智坤\", \"殘障\", \"求職\", \"不易\"]\n",
      "[\"洪智坤\", \"人事\", \"處議\", \"清楚\", \"議處\", \"結果\"]\n",
      "=========== TfidfVectorizer ===========\n",
      "[\"今處理\", \"看到\", \"案還\", \"公文\", \"洩漏\", \"洪智坤\", \"更新\"]\n",
      "[\"不易\", \"求職\", \"殘障\", \"洪智坤\"]\n",
      "[\"結果\", \"議處\", \"清楚\", \"處議\", \"人事\", \"洪智坤\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"=========== CountVectorizer ===========\")\n",
    "print (json.dumps([u'\\u66f4\\u65b0', u'\\u6d2a\\u667a\\u5764', u'\\u6d29\\u6f0f',\n",
    "       u'\\u516c\\u6587', u'\\u6848\\u9084', u'\\u770b\\u5230',\n",
    "       u'\\u4eca\\u8655\\u7406'],ensure_ascii=False))\n",
    "print (json.dumps([u'\\u6d2a\\u667a\\u5764', u'\\u6b98\\u969c', u'\\u6c42\\u8077',\n",
    "       u'\\u4e0d\\u6613'],ensure_ascii=False))\n",
    "print (json.dumps([u'\\u6d2a\\u667a\\u5764', u'\\u4eba\\u4e8b', u'\\u8655\\u8b70',\n",
    "       u'\\u6e05\\u695a', u'\\u8b70\\u8655', u'\\u7d50\\u679c'],ensure_ascii=False))\n",
    "print(\"=========== TfidfVectorizer ===========\")\n",
    "print (json.dumps([u'\\u4eca\\u8655\\u7406', u'\\u770b\\u5230', u'\\u6848\\u9084',\n",
    "       u'\\u516c\\u6587', u'\\u6d29\\u6f0f', u'\\u6d2a\\u667a\\u5764',\n",
    "       u'\\u66f4\\u65b0'],ensure_ascii=False))\n",
    "print (json.dumps([u'\\u4e0d\\u6613', u'\\u6c42\\u8077', u'\\u6b98\\u969c',\n",
    "       u'\\u6d2a\\u667a\\u5764'],ensure_ascii=False))\n",
    "print (json.dumps([u'\\u7d50\\u679c', u'\\u8b70\\u8655', u'\\u6e05\\u695a', u'\\u8655\\u8b70',\n",
    "       u'\\u4eba\\u4e8b', u'\\u6d2a\\u667a\\u5764'],ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureHasher\n",
    "- hasher = FeatureHasher(n_features=10): 後面hasher.fit_transform()要丟list(dictionay)\n",
    "- hasher = FeatureHasher(n_features=10,input_type=\"string\"): 後面hasher.fit_transform要丟list(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureHasher: \n",
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t-2.0\n",
      "  (1, 7)\t-3.0\n",
      "  (1, 9)\t4.0\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "hasher = FeatureHasher(n_features=10)\n",
    "featureHasher=hasher.fit_transform([{'a':1,'b':2},{'c':3,'d':4}])\n",
    "print (\"featureHasher: \")\n",
    "print (featureHasher)\n",
    "print (type(featureHasher))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'scipy.sparse.csr.csr_matrix' v.s. 'numpy.ndarray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== scipy.sparse.csr.csr_matrix ============   (0, 2)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 14)\t1\n",
      "============= numpy.ndarray ============== [[0 0 1 2 1 1 0 0 1 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# from 'numpy.ndarray' to 'scipy.sparse.csr.csr_matrix' \n",
    "from scipy import sparse\n",
    "print \"=========== scipy.sparse.csr.csr_matrix ============\",\n",
    "print sparse.csr_matrix(X.toarray())   \n",
    "\n",
    "# from 'scipy.sparse.csr.csr_matrix' to 'numpy.ndarray'\n",
    "print \"============= numpy.ndarray ==============\",\n",
    "print X.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
