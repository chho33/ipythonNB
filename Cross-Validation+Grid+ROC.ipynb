{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train(70%), Validation(15%), & Test (15%)\n",
    "Validation set actually can be regarded as a part of training set, \n",
    "because it is used to build your model, used for parameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GridSearch(est, param_grid, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#一些需要的function\n",
    "import numpy as np\n",
    "def gauss(x,m,s):\n",
    "    e=-(x-m)**2/(2*(s**2))\n",
    "    g=1*np.exp(e)/(2*s*np.pi)**0.5\n",
    "    return g\n",
    "\n",
    "def remove_punc(sentences):\n",
    "    import string \n",
    "    import re\n",
    "    delEStr = '['+string.punctuation + u' '+']'\n",
    "    delCStr = u'''[〔△〇◎◆●✦×→°☆■□◆●○๏••‧㊣℃ˍ╴←↑→．│﹙“—▲►★《》ΟΔΧ╳（）()∫&%﹪￥#@$！?,.…><*{}｢｣\\\\∼〈〉¬﹄•∼■`\"’“×※【】  ’'　、。，‧．·‧‧：︰十；”～‘’′→＠＃＄％︿＆＊｀＝－—─＋｜／＼╲╱╱ ╱╱？『』」「\\/]'''\n",
    "    s=[]\n",
    "    for sentence in sentences:\n",
    "        if type(sentence)==unicode:\n",
    "            sentence=re.sub(delEStr,u'',sentence)\n",
    "            sentence=re.sub(delCStr,u'',sentence)\n",
    "            s.append(sentence.encode('utf-8'))\n",
    "        else:\n",
    "            sentence=re.sub(delEStr,u'',sentence.decode('utf-8'))\n",
    "            sentence=re.sub(delCStr,u'',sentence)\n",
    "            s.append(sentence.encode('utf-8'))\n",
    "    return s  \n",
    "\n",
    "\n",
    "def replace_punc(sentences):\n",
    "    import string \n",
    "    import re\n",
    "    delEStr = '['+string.punctuation + u' '+']'\n",
    "    delCStr = u'''[〔△〇◎◆●✦×→°☆■□◆●○๏••‧㊣℃ˍ╴←↑→．│﹙“—▲►★《》ΟΔΧ╳（）()∫&%﹪￥#@$！?,.…><*{}｢｣\\\\∼〈〉¬﹄•∼■`\"’“×※【】  ’'　、。，‧．·‧‧：︰十；”～‘’′→＠＃＄％︿＆＊｀＝－—─＋｜／＼╲╱╱ ╱╱？『』」「\\/]'''\n",
    "    s=[]\n",
    "    for sentence in sentences:\n",
    "        if type(sentence)==unicode:\n",
    "            sentence=re.sub(delEStr,u' ',sentence)\n",
    "            sentence=re.sub(delCStr,u' ',sentence).encode('utf-8')\n",
    "            sentence=sentence.split(\",\")\n",
    "            #s.append(sentence.encode('utf-8'))\n",
    "            s.append(sentence)\n",
    "        else:\n",
    "            sentence=re.sub(delEStr,u' ',sentence.decode('utf-8'))\n",
    "            sentence=re.sub(delCStr,u' ',sentence).encode('utf-8')\n",
    "            sentence=sentence.split(\",\")\n",
    "            #s.append(sentence.encode('utf-8'))\n",
    "            s.append(sentence)\n",
    "    return s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /usr/local/lib/python2.7/site-packages/jieba/dictbig.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /usr/local/lib/python2.7/site-packages/jieba/dictbig.txt ...\n",
      "Loading model from cache /var/folders/pn/qc_lny8n1cv6pt_2tqlg4s6m0000gn/T/jieba.u91a3eae8b085aaf1cd1517c2fdfc2047.cache\n",
      "DEBUG:jieba:Loading model from cache /var/folders/pn/qc_lny8n1cv6pt_2tqlg4s6m0000gn/T/jieba.u91a3eae8b085aaf1cd1517c2fdfc2047.cache\n",
      "Loading model cost 1.023 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.023 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-9c9b55f749b4>\", line 117, in <module>\n",
      "    cur2.execute('insert into keywords(keyid,term,weight,freq) values(%s,%s,%s,%s)',data)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/MySQLdb/cursors.py\", line 205, in execute\n",
      "    self.errorhandler(self, exc, value)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/MySQLdb/connections.py\", line 36, in defaulterrorhandler\n",
      "    raise errorclass, errorvalue\n",
      "IntegrityError: (1062, \"Duplicate entry '1-\\xe7\\x8f\\xbe\\xe4\\xbb\\xbb' for key 'PRIMARY'\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bashCommand = \"rm test.txt\"\n",
    "import subprocess\n",
    "subprocess.Popen(bashCommand.split(),cwd='/Users/jojotenya/Desktop/gensim/')\n",
    "\n",
    "# 逐行從資料庫提出文章>斷詞\n",
    "import MySQLdb as mc\n",
    "import MySQLdb.cursors\n",
    "import traceback\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from gensim import corpora, models, similarities\n",
    "import json\n",
    "import time \n",
    "\n",
    "############# 基本參數設定 ###############\n",
    "host='localhost'\n",
    "user='root'\n",
    "db=\"ecodaily3\"\n",
    "passwd=\"\"\n",
    "newstable='news2' #記得下面有個地方要改\n",
    "########################################\n",
    "\n",
    "jieba.set_dictionary('/usr/local/lib/python2.7/site-packages/jieba/dictbig.txt')\n",
    "jieba.load_userdict(\"userdict.txt\")\n",
    "conn=mc.connect(host='localhost',user='root',passwd='',db=db,charset=\"utf8\",cursorclass = MySQLdb.cursors.SSCursor)\n",
    "cur=conn.cursor()\n",
    "sql='select content from news2'\n",
    "cur.execute(sql)\n",
    "f=open('/Users/jojotenya/Desktop/gensim/test.txt','a')\n",
    "texts=cur.fetchone()\n",
    "s=open('/Users/jojotenya/Desktop/gensim/stopword.txt',\"r\")\n",
    "stoplist=[]\n",
    "for stop in s.readlines():\n",
    "    stoplist.append(stop.strip())\n",
    "s.close()\n",
    "stoplist=set(stoplist)\n",
    "while texts is not None:\n",
    "    words = jieba.cut(replace_punc([texts[0]])[0], cut_all=False)\n",
    "    sentence=[]\n",
    "    for w in words:\n",
    "        if len(w.strip())<=1:\n",
    "            continue\n",
    "        elif w.encode('utf-8') in stoplist:\n",
    "            #print \"stop: \", w\n",
    "            continue\n",
    "        else:\n",
    "            #print \"enter: \",w\n",
    "            sentence.append(w)\n",
    "    feed=(' '.join(sentence)).encode('utf-8')\n",
    "    f.write(feed)\n",
    "    f.write(\"\\n\")\n",
    "    texts=cur.fetchone()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "f.close()\n",
    "\n",
    "\n",
    "#轉換斷詞為純數字vector，生成dictionary詞庫、詞頻統計\n",
    "#stoplist=[]\n",
    "dictionary = corpora.Dictionary(line.split() for line in open('/Users/jojotenya/Desktop/gensim/test.txt'))\n",
    "#stop_ids=[]\n",
    "#for stopword in stoplist:\n",
    "#    if stopword.decode('utf-8') in dictionary.token2id:\n",
    "#        stop_ids.append(stopword.decode('utf-8'))\n",
    "#once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
    "#once_ids = []\n",
    "#dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
    "#dictionary.filter_tokens(set(stop_ids))\n",
    "#dictionary.compactify()\n",
    "dictionary.save('/Users/jojotenya/Desktop/gensim/test.dict')\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('/Users/jojotenya/Desktop/gensim/test.txt'):\n",
    "            yield dictionary.doc2bow(line.split())\n",
    "corpus = MyCorpus() \n",
    "corpora.MmCorpus.serialize('/Users/jojotenya/Desktop/gensim/test.mm', corpus)\n",
    "\n",
    "'''for c in corpus_to_use:\n",
    "    print json.dumps(c,ensure_ascii=False)'''\n",
    "\n",
    "#做tfidf，將斷詞編號依照vector轉換回原本的詞，一同將詞與權重存入database\n",
    "#Lsi降維，將各topic與權重存入database\n",
    "#print json.dumps(dictionary.token2id, encoding=\"utf-8\", ensure_ascii=False)\n",
    "##no update：\n",
    "#dictionary=corpora.Dictionary.load('/Users/jojotenya/Desktop/test.dict')\n",
    "#corpus=corpora.MmCorpus('/Users/jojotenya/Desktop/test.dict')\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "t=[]\n",
    "for doc in corpus_tfidf:\n",
    "     t.append([[dictionary.token2id.keys()[dictionary.token2id.values().index(d[0])],d[1]] for d in doc])      \n",
    "\n",
    "c=[]\n",
    "for doc in corpus:\n",
    "     c.append([d[1] for d in doc])\n",
    "\n",
    "for i in xrange(0,60):\n",
    "    for j in xrange(0,len(t[i])):\n",
    "        t[i][j].append(c[i][j])     \n",
    "        \n",
    "try:\n",
    "    conn=mc.connect(host='localhost',user='root',passwd='',db=db,charset=\"utf8\",cursorclass = MySQLdb.cursors.SSCursor)\n",
    "    cur=conn.cursor()\n",
    "    sql='select newsID from news2'\n",
    "    cur.execute(sql)\n",
    "    row=cur.fetchone()\n",
    "    conn2=mc.connect(host='localhost',user='root',passwd='',db=db,charset=\"utf8\")\n",
    "    conn2.query('SET autocommit=TRUE;')\n",
    "    cur2=conn2.cursor()\n",
    "    #print json.dumps(t[i],ensure_ascii=False)\n",
    "    i=0\n",
    "    while row is not None:    \n",
    "        for s in t[i]:  \n",
    "            data=(int(row[0]),s[0],s[1],s[2])\n",
    "            cur2.execute('insert into keywords(keyid,term,weight,freq) values(%s,%s,%s,%s)',data)\n",
    "            #conn2.commit()\n",
    "        for c in l[i]: \n",
    "            data=(int(row[0]),c[0],c[1])\n",
    "            cur3.execute('insert into lsi(lsiid,topic,weight) values(%s,%s,%s)',data)\n",
    "            #conn3.commit()\n",
    "        i+=1\n",
    "        row=cur.fetchone()\n",
    "except:\n",
    "    print traceback.format_exc()\n",
    "\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    cur2.close()\n",
    "    conn2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CV to find best lsi - GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.270159\n",
      "0.265904\n",
      "0.352915\n",
      "0.389943\n",
      "0.400208\n",
      "0.391812\n",
      "0.41543\n",
      "0.420937\n",
      "0.43689\n",
      "0.51424\n",
      "0.384048\n",
      "0.386632\n",
      "0.392296\n",
      "0.392051\n",
      "0.4114\n",
      "0.483309\n",
      "0.468649\n",
      "0.44848\n",
      "0.516124\n",
      "0.485424\n",
      "0.464713\n"
     ]
    }
   ],
   "source": [
    "############# 基本參數設定 ###############\n",
    "host='localhost'\n",
    "user='root'\n",
    "db=\"ecodaily3\"\n",
    "passwd=\"\"\n",
    "newstable='news2'  #記得下面有個地方要改\n",
    "rang1=5\n",
    "rang2=26\n",
    "#num_topics=10\n",
    "########################################\n",
    "meanstd=[]\n",
    "for num_topics in xrange(rang1,rang2):\n",
    "    #lsi\n",
    "    a= time.clock()\n",
    "\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "    lsi.save('/Users/jojotenya/Desktop/gensim/test.lsi')\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    l=[]\n",
    "    for c_lsi in corpus_lsi:\n",
    "        l.append(c_lsi)  \n",
    "    try:\n",
    "        conn=mc.connect(host=host,user=user,passwd=passwd,db=db,charset=\"utf8\",cursorclass = MySQLdb.cursors.SSCursor)\n",
    "        cur=conn.cursor()\n",
    "        sql_drop_lsi=\"drop table if exists lsi;\"\n",
    "        cur.execute(sql_drop_lsi)\n",
    "        conn.commit()\n",
    "        sql_create_lsi='''\n",
    "        create table lsi(\n",
    "        lsiid int not null,\n",
    "        topic int not null,\n",
    "        weight float,\n",
    "        primary key pk_lsi (lsiid,topic)\n",
    "        );\n",
    "        '''\n",
    "        cur.execute(sql_create_lsi)\n",
    "        conn.commit()\n",
    "\n",
    "        sql_drop_lsiStatistic=\"drop table if exists lsiStatistic;\"\n",
    "        cur.execute(sql_drop_lsiStatistic)\n",
    "        conn.commit()\n",
    "        sql_create_lsiStatistic='''\n",
    "            create table lsiStatistic(\n",
    "            `class` varchar(10),\n",
    "            topic int not null,\n",
    "            mean float,\n",
    "            std float,\n",
    "            primary key pk_lsiStatistic (`class`,topic)\n",
    "            );\n",
    "        '''\n",
    "        cur.execute(sql_create_lsiStatistic)\n",
    "        conn.commit()\n",
    "\n",
    "        sql='select newsID from %s'%newstable\n",
    "        cur.execute(sql)\n",
    "        row=cur.fetchone()\n",
    "        conn3=mc.connect(host=host,user=user,passwd=passwd,db=db,charset=\"utf8\")\n",
    "        conn3.query('SET autocommit=TRUE;')\n",
    "        cur3=conn3.cursor()\n",
    "        i=0\n",
    "        while row is not None:    \n",
    "            for c in l[i]: \n",
    "                data=(int(row[0]),c[0],c[1])\n",
    "                cur3.execute('insert into lsi(lsiid,topic,weight) values(%s,%s,%s)',data)\n",
    "                #conn3.commit()\n",
    "            i+=1\n",
    "            row=cur.fetchone()\n",
    "    except:\n",
    "        print traceback.format_exc()\n",
    "\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        cur3.close()\n",
    "        conn3.close()\n",
    "\n",
    "    #每個分類的lsi-mu,lsi-sigma\n",
    "    conn=mc.connect(host='localhost',user='root',passwd='',\n",
    "                    db=db,charset=\"utf8\",cursorclass = MySQLdb.cursors.SSCursor)\n",
    "    cur=conn.cursor()\n",
    "    cur.execute('select distinct(`class`) from %s'%newstable)\n",
    "    categories=cur.fetchall()\n",
    "\n",
    "    conn2=mc.connect(host='localhost',user='root',passwd='',\n",
    "                    db=db,charset=\"utf8\",cursorclass = MySQLdb.cursors.SSCursor)\n",
    "    conn2.query('SET autocommit=TRUE;')\n",
    "    cur2=conn2.cursor()\n",
    "\n",
    "    for i in xrange(0,len(categories)):\n",
    "        cur.execute('select topic,AVG(weight),STD(weight) from lsi where lsiid in \\\n",
    "                    (select newsID from news2 where class = %s) \\\n",
    "                    GROUP BY topic',(categories[i][0],))\n",
    "        result=cur.fetchone()\n",
    "        while result is not None:\n",
    "            data=(categories[i],result[0],result[1],result[2])\n",
    "            cur2.execute('insert into lsiStatistic(`class`,topic,mean,std) value(%s,%s,%s,%s)',data)\n",
    "            #conn2.commit()\n",
    "            result=cur.fetchone()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    cur2.close()\n",
    "    conn2.close()\n",
    "\n",
    "    # create train set and test it\n",
    "    conn=mc.connect(host=host,user=user,passwd=passwd,\n",
    "                    db=db,charset=\"utf8\",cursorclass = MySQLdb.cursors.SSCursor)\n",
    "    cur=conn.cursor()\n",
    "    cur.execute('select newsID,reclass from %s where reclass is not null '%newstable)\n",
    "    idClass=[i for i in cur.fetchall()]\n",
    "    ID=[i[0] for i in idClass]\n",
    "    classSetRaw=[i[1] for i in idClass]\n",
    "    categories=set(classSetRaw)\n",
    "    classdic={}\n",
    "    for i,j in enumerate(categories):\n",
    "        classdic[j]=i\n",
    "    classSet=[classdic[c] for c in classSetRaw]     #for train and test\n",
    "    dataSet=[]\n",
    "    for i in ID:\n",
    "        cur.execute('select weight from lsi where lsiid = %s'%i)\n",
    "        ds=[i[0] for i in cur.fetchall()]\n",
    "        dataSet.append(ds)\n",
    "\n",
    "    classSet=np.array(classSet)\n",
    "    dataSet=np.array(dataSet)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    dataTrain=[]\n",
    "    dataTestSet=[]\n",
    "    classTrain=[]\n",
    "    classTestSet=[]\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "    skf = StratifiedKFold(classSet, 10,shuffle=False, random_state=0)  \n",
    "    for train_index, test_index in skf:\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        dataTrain.append(dataSet[train_index])\n",
    "        dataTestSet.append(dataSet[test_index])\n",
    "        classTrain.append(classSet[train_index])\n",
    "        classTestSet.append(classSet[test_index])\n",
    "    #print classTrain[0].shape\n",
    "    #print dataTrain[0].shape\n",
    "\n",
    "    '''from sklearn.cross_validation import train_test_split \n",
    "    for i in xrange(0,10):\n",
    "        X=dataTrain[i]\n",
    "        y=classTrain[i]\n",
    "        dataTrain, dataValidation, classTrain, classValidation = train_test_split(X,y, test_size=0.22, random_state=0)  #X放資料,y放類別,type:nparray \n",
    "    #    print(\"train data shape: %r, train target shape: %r\"\n",
    "    #          % (dataTrain.shape, classTrain.shape)) # train data shape: (1437, 64), train target shape: (1437,)\n",
    "    #    print(\"validation data shape: %r, validation target shape: %r\"\n",
    "    #          % (dataValidation.shape, classValidation.shape)) # test data shape: (360, 64), test target shape: (360,)'''\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split \n",
    "    dataTrainSet=[]\n",
    "    dataValSet=[]\n",
    "    classTrainSet=[]\n",
    "    classValSet=[]\n",
    "    for i in range(0,10):\n",
    "        X=dataTrain[i]\n",
    "        y=classTrain[i]\n",
    "        dt, dv, ct, cv = train_test_split(X,y, test_size=0.22, random_state=0)  #X放資料,y放類別,type:nparray \n",
    "        dataTrainSet.append(dt)\n",
    "        dataValSet.append(dv)\n",
    "        classTrainSet.append(ct)\n",
    "        classValSet.append(cv)\n",
    "        #print(\"train data shape: %r, train target shape: %r\"% (dt.shape, ct.shape)) # train data shape: (1437, 64), train target shape: (1437,)\n",
    "        #print(\"validation data shape: %r, validation target shape: %r\"% (dv.shape, cv.shape))\n",
    "    dataTrainSet=np.array(dataTrainSet)\n",
    "    dataValSet=np.array(dataValSet)\n",
    "    classTrainSet=np.array(classTrainSet)\n",
    "    classValSet=np.array(classValSet)\n",
    "\n",
    "    from sklearn.naive_bayes import GaussianNB    \n",
    "    import pandas as pd\n",
    "    scores=[]\n",
    "\n",
    "    for i in range(0,10):\n",
    "        dic={}\n",
    "        clf = GaussianNB()\n",
    "        clf.fit(dataTrainSet[i], classTrainSet[i])\n",
    "        train_score = clf.score(dataTrainSet[i], classTrainSet[i])\n",
    "        #print train_scores\n",
    "        val_score = clf.score(dataValSet[i], classValSet[i])\n",
    "        #print val_scores\n",
    "        test_score = clf.score(dataTestSet[i], classTestSet[i])\n",
    "        #print test_scores\n",
    "        #print \"---\"\n",
    "        #f.write('\\t-fold'+str(i)+': \\n')\n",
    "        #f.write('\\ttrain_score: '+str(train_score)+'\\n')\n",
    "        #f.write('\\tvalidation_score: '+str(val_score)+'\\n')\n",
    "        #f.write('\\tttest_score: '+str(test_score)+'\\n')\n",
    "        #train_scores.append(train_score)\n",
    "        #val_scores.append(val_score)\n",
    "        #test_scores.append(test_score)\n",
    "        dic['train_scores']=train_score\n",
    "        dic['val_scores']=val_score\n",
    "        dic['test_scores']=test_score\n",
    "        scores.append(dic)\n",
    "    df = pd.DataFrame(scores)\n",
    "    #df.to_csv('/Users/jojotenya/Desktop/GaussNB/GaussNB'+'_'+str(num_topics)+'.csv',encoding=\"utf-8\")\n",
    "\n",
    "    \n",
    "    # 算CV的mean,std\n",
    "    from scipy.stats import sem\n",
    "    train_scores=[dic['train_scores'] for dic in scores]\n",
    "    val_scores =[dic['val_scores'] for dic in scores]\n",
    "    test_scores=[dic['test_scores'] for dic in scores]\n",
    "    train_scores_mean=np.mean(train_scores)\n",
    "    val_scores_mean=np.mean(val_scores)\n",
    "    test_scores_mean=np.mean(test_scores)\n",
    "    total_mean=np.mean(test_scores+train_scores+val_scores)\n",
    "    train_scores_std=sem(train_scores)\n",
    "    val_scores_std=sem(val_scores)\n",
    "    test_scores_std=sem(test_scores)\n",
    "    total_std=sem(test_scores+train_scores+val_scores)  \n",
    "    meandic={}\n",
    "    meandic[\"num_topics\"]=num_topics\n",
    "    meandic[\"Mean(train)\"]=train_scores_mean\n",
    "    meandic[\"Mean(validation)\"]=val_scores_mean\n",
    "    meandic[\"Mean(test)\"]=test_scores_mean\n",
    "    meandic[\"Mean(total)\"]=total_mean\n",
    "    meandic[\"std(train)\"]=train_scores_std\n",
    "    meandic[\"std(validation)\"]=val_scores_std\n",
    "    meandic[\"std(test)\"]=test_scores_std\n",
    "    meandic[\"std(total)\"]=total_std\n",
    "    meanstd.append(meandic)\n",
    "    #print '========================================='\n",
    "    #print 'num_topics: ',num_topics\n",
    "    #print df\n",
    "    #print (\"Mean score(train): {0:.3f} (+/-{1:.3f})\").format(train_scores_mean,train_scores_std)\n",
    "    #print (\"Mean score(validation): {0:.3f} (+/-{1:.3f})\").format(val_scores_mean,val_scores_std)\n",
    "    #print (\"Mean score(test): {0:.3f} (+/-{1:.3f})\").format(test_scores_mean,test_scores_std)\n",
    "    #print '------------------------------------'\n",
    "    #print (\"Mean score(total): {0:.3f} (+/-{1:.3f})\").format(total_mean,total_std)  \n",
    "    print time.clock()-a\n",
    "df2=pd.DataFrame(meanstd)\n",
    "df2.to_csv('/Users/jojotenya/Desktop/GaussNB/GaussNB'+'_'+str(rang1)+'_'+str(rang2-1)+'.csv',encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CV to find best lsi - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CV to find best lsi - SVM -ROC -tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-validation:\n"
     ]
    }
   ],
   "source": [
    "# SVM-ROC\n",
    "####################\n",
    "n_gammas = 5\n",
    "gammas = np.logspace(-7, -1, n_gammas)\n",
    "n_Cs = 5\n",
    "Cs = np.logspace(-5, 5, n_Cs)\n",
    "####################\n",
    "\n",
    "svmcsv=[]\n",
    "print \"train-validation:\"\n",
    "for num_topics in xrange(rang1,rang2):\n",
    "    for l,gamma in enumerate(gammas):\n",
    "        for m,C in enumerate(Cs):\n",
    "            conn=mc.connect(host=host,user=user,passwd=passwd,\n",
    "                            db=db,charset=\"utf8\",cursorclass = MySQLdb.cursors.SSCursor)\n",
    "            cur=conn.cursor()\n",
    "            cur.execute('select newsID,reclass from %s where reclass is not null '%newstable)\n",
    "            idClass=[i for i in cur.fetchall()]\n",
    "            ID=[i[0] for i in idClass]\n",
    "            classSetRaw=[i[1] for i in idClass]\n",
    "            categories=set(classSetRaw)\n",
    "            classdic={}\n",
    "            for i,j in enumerate(categories):\n",
    "                classdic[j]=i\n",
    "            classSet=[classdic[c] for c in classSetRaw]     #for train and test\n",
    "            dataSet=[]\n",
    "            for i in ID:\n",
    "                cur.execute('select weight from lsi where lsiid = %s'%i)\n",
    "                ds=[i[0] for i in cur.fetchall()]\n",
    "                dataSet.append(ds)\n",
    "\n",
    "            classSet=np.array(classSet)\n",
    "            dataSet=np.array(dataSet)\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "\n",
    "            dataTrain=[]\n",
    "            dataTestSet=[]\n",
    "            classTrain=[]\n",
    "            classTestSet=[]\n",
    "            skf = StratifiedKFold(classSet, 10,shuffle=False, random_state=0)  \n",
    "            for train_index, test_index in skf:\n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                dataTrain.append(dataSet[train_index])\n",
    "                dataTestSet.append(dataSet[test_index])\n",
    "                classTrain.append(classSet[train_index])\n",
    "                classTestSet.append(classSet[test_index])\n",
    "            #print classTrain[0].shape\n",
    "            #print dataTrain[0].shape\n",
    "\n",
    "\n",
    "            dataTrainSet=[]\n",
    "            dataValSet=[]\n",
    "            classTrainSet=[]\n",
    "            classValSet=[]\n",
    "            for i in range(0,10):\n",
    "                X=dataTrain[i]\n",
    "                y=classTrain[i]\n",
    "                dt, dv, ct, cv = train_test_split(X,y, test_size=0.22, random_state=0)  #X放資料,y放類別,type:nparray \n",
    "                dataTrainSet.append(dt)\n",
    "                dataValSet.append(dv)\n",
    "                classTrainSet.append(ct)\n",
    "                classValSet.append(cv)\n",
    "                #print(\"train data shape: %r, train target shape: %r\"% (dt.shape, ct.shape)) # train data shape: (1437, 64), train target shape: (1437,)\n",
    "                #print(\"validation data shape: %r, validation target shape: %r\"% (dv.shape, cv.shape))\n",
    "            dataTrainSet=np.array(dataTrainSet)\n",
    "            dataValSet=np.array(dataValSet)\n",
    "            classTrainSet=np.array(classTrainSet)\n",
    "            classValSet=np.array(classValSet)\n",
    "\n",
    "            n_classes=len(set(classSet))\n",
    "            from sklearn.preprocessing import label_binarize\n",
    "            classTrainSet_b=[label_binarize(d, classes=list(set(classSet))) for d in classTrainSet]\n",
    "            classValSet_b=[label_binarize(d, classes=list(set(classSet))) for d in classValSet]\n",
    "            classTestSet_b=[label_binarize(d, classes=list(set(classSet))) for d in classTestSet]\n",
    "\n",
    "            #-----------------------\n",
    "            from sklearn import svm, datasets\n",
    "            from sklearn.metrics import roc_curve, auc\n",
    "            from sklearn.cross_validation import train_test_split\n",
    "            from sklearn.multiclass import OneVsRestClassifier\n",
    "            X_train=dataTrainSet[0]\n",
    "            y_tr=classTrainSet_b[0]\n",
    "            y_train=[]\n",
    "            for y in y_tr:\n",
    "                if y == [0]:\n",
    "                    y_train.append([1,0])\n",
    "                elif y == [1]:\n",
    "                    y_train.append([0,1])\n",
    "                else:\n",
    "                    y_train.append(y)\n",
    "                    \n",
    "            X_test=dataValSet[0]\n",
    "            y_te=classValSet_b[0]\n",
    "            y_test=[]\n",
    "            for y in y_te:\n",
    "                if y == [0]:\n",
    "                    y_test.append([1,0])\n",
    "                elif y == [1]:\n",
    "                    y_test.append([0,1])\n",
    "                else:\n",
    "                    y_test.append(y)\n",
    "            y_train=np.array(y_train)\n",
    "            y_test=np.array(y_test)\n",
    "            #print X_train.shape\n",
    "            #print y_train.shape\n",
    "            #print X_test.shape\n",
    "            #print y_train\n",
    "            #print y_test\n",
    "            # Learn to predict each class against the other\n",
    "            classifier = OneVsRestClassifier(svm.SVC(C=C,gamma=gamma,kernel='linear', probability=True,\n",
    "            random_state=0))\n",
    "            y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "            #print y_score\n",
    "\n",
    "            # Compute ROC curve and ROC area for each class\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            for i in range(n_classes):\n",
    "                fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            # Compute micro-average ROC curve and ROC area\n",
    "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "            # Plot ROC curve\n",
    "            plt.figure()\n",
    "            plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                     label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"micro\"]))\n",
    "            for i in range(n_classes):\n",
    "                plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                               ''.format(i, roc_auc[i]))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('1-SPC')\n",
    "            plt.ylabel('TPR')\n",
    "            plt.title('ROC:num_topics='+str(num_topics)+',gamma='+str(gamma)+',C='+str(C))\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig('/Users/jojotenya/Desktop/ROC/ROC_SVM_'+str(num_topics)+str(l)+str(m)+'.png',bbox_inches='tight')\n",
    "            plt.close()\n",
    "            #plt.show()\n",
    "            svmpreffix={}\n",
    "            svmpreffix[\"num_topics\"]=num_topics\n",
    "            svmpreffix[\"gamma\"]=gamma\n",
    "            svmpreffix[\"C\"]=C\n",
    "            svmcsv.append(dict(svmpreffix.items()+roc_auc.items()))\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(svmcsv)\n",
    "df.to_csv('/Users/jojotenya/Desktop/ROC/SVM'+'_'+str(rang1)+'_'+str(rang2-1)+'.csv',encoding=\"utf-8\")\n",
    "#print 'num_topics: ',num_topics\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CV to find best lsi - SVM -ROC -test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "[[-0.76305896 -0.36472635  0.1239796 ]\n",
      " [-0.20238399 -0.63148982 -0.16616656]\n",
      " [ 0.11808492 -0.80262259 -0.32062486]\n",
      " [-0.90750303 -0.1239792   0.02184016]\n",
      " [-0.01108555 -0.27918155 -0.71882525]\n",
      " [-0.60521037 -0.34728075 -0.05851787]\n",
      " [ 0.02264569 -0.24507866 -0.79094201]\n",
      " [-0.61115098  0.1827259  -0.57154703]\n",
      " [-0.37590649 -0.24062697 -0.38892819]\n",
      " [-0.47019698 -0.25733679 -0.27501927]\n",
      " [-0.42194767 -0.30275351 -0.28027547]\n",
      " [-0.33545871 -0.70306393  0.02538619]\n",
      " [-0.22711497 -0.64064389 -0.13452752]\n",
      " [-0.07836815 -0.46342889 -0.45930643]\n",
      " [-0.533757   -0.26521263 -0.20026826]\n",
      " [ 0.09973063 -0.55326984 -0.56156984]\n",
      " [-0.71354879 -0.04231832 -0.24317009]\n",
      " [-0.55067912 -0.13785396 -0.31413579]\n",
      " [ 0.37989555 -0.99673681 -0.3909524 ]\n",
      " [-0.11088917 -0.91357544  0.03129667]\n",
      " [-0.70721114 -0.06430956 -0.21412904]\n",
      " [-0.02407429 -0.45895598 -0.51928682]\n",
      " [-0.25004251 -0.80100991  0.04076059]\n",
      " [ 0.12688003 -0.70987418 -0.41083444]\n",
      " [-0.68224184 -0.20724969 -0.1202643 ]\n",
      " [-0.0800568  -0.36695547 -0.57664396]\n",
      " [-0.03254409 -0.11603038 -0.8648849 ]\n",
      " [-0.04974597 -0.73610513 -0.21652326]\n",
      " [-0.13003384 -0.37669991 -0.49962148]\n",
      " [-0.19289268 -0.71079054 -0.11051706]\n",
      " [-0.36221002 -0.41410723 -0.2273942 ]\n",
      " [-0.2284516  -0.78973024  0.02074984]\n",
      " [-0.06178684 -0.47628223 -0.45378569]\n",
      " [-0.52446923 -0.46496654 -0.00359355]\n",
      " [-0.40029714 -0.71472476  0.10099661]\n",
      " [-0.3511887  -0.31112596 -0.33970555]\n",
      " [-0.05774206 -0.51400526 -0.41773929]\n",
      " [-1.11908314 -0.00736339  0.12950484]\n",
      " [ 0.19597017 -0.65775051 -0.54614202]\n",
      " [-0.04342577 -0.60042226 -0.35868612]\n",
      " [-0.4812135  -0.21920638 -0.30068446]\n",
      " [ 0.17431275 -1.0108599  -0.18121717]\n",
      " [-0.41431727 -0.60032358  0.00863558]\n",
      " [-0.01086869 -0.75802709 -0.22892428]\n",
      " [ 0.01626312 -0.8156017  -0.20376516]\n",
      " [-0.11923774 -0.84788528 -0.05836804]\n",
      " [-0.70843733 -0.286195   -0.01169663]\n",
      " [-0.77322168 -0.43208966  0.21330137]\n",
      " [-0.61461496 -0.15074926 -0.23335428]\n",
      " [-0.96335686 -0.62814722  0.58431039]\n",
      " [-0.31000437 -0.29574607 -0.3941874 ]\n",
      " [-0.31988026 -0.34629128 -0.32066024]\n",
      " [-0.35307167 -0.66921938  0.00761443]\n",
      " [ 0.12141618 -0.62483742 -0.50579325]\n",
      " [-0.66412149 -0.11447286 -0.21316263]\n",
      " [-0.55119677 -0.34855218 -0.10579228]\n",
      " [-0.55717268 -0.13386492 -0.30604104]\n",
      " [-0.41121128 -0.52473406 -0.07437911]\n",
      " [-0.49442008 -0.2332585  -0.27820574]\n",
      " [ 0.06881717 -0.85437126 -0.21661728]\n",
      " [-0.23058968 -0.48751709 -0.28302624]\n",
      " [ 0.30309598 -0.83392326 -0.47723277]\n",
      " [ 0.17584119 -0.81582683 -0.37510759]\n",
      " [-0.19111926 -0.70824907 -0.10203375]\n",
      " [-0.42896846 -0.3990632  -0.16931316]\n",
      " [-0.67758109  0.09202727 -0.39954675]\n",
      " [-0.32984013 -0.56568558 -0.1204075 ]\n",
      " [-0.97144386 -0.46477124  0.41494993]\n",
      " [ 0.02097654 -0.56915133 -0.44612973]\n",
      " [-0.08193282 -0.56128644 -0.35207865]\n",
      " [-0.41489593 -0.63695943  0.04859412]\n",
      " [-0.30434286 -0.08424982 -0.61862499]\n",
      " [ 0.18874747 -0.88796087 -0.29723377]\n",
      " [ 0.24975994 -0.80511827 -0.44319443]\n",
      " [-0.39991848 -0.29010996 -0.30381285]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
    "                                                    random_state=0)\n",
    "\n",
    "'''print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_train'''\n",
    "# Learn to predict each class against the other\n",
    "#print y_train\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=0))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "#print y_score\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decide gamma\n",
    "n_gammas = 10\n",
    "n_iter = 5\n",
    "cv = ShuffleSplit(n_samples, n_iter=n_iter, train_size=500, test_size=500, random_state=0)\n",
    "train_scores = np.zeros((n_gammas, n_iter))  #創造array空間 5*10\n",
    "test_scores = np.zeros((n_gammas, n_iter))\n",
    "gammas = np.logspace(-7, -1, n_gammas)\n",
    "#create gammas array，稍微有了解過 libsvm 的人, 會知道要帶入 gamma 的差距通常會給上下界\n",
    "#然後抓 log 曲線(也就是一般帶數字是 1,2,3,4 彼此差 1 他則是從 10-7帶到10-1, 彼此差 log 距離)\n",
    " \n",
    "for i, gamma in enumerate(gammas):\n",
    "    for j, (train, test) in enumerate(cv):\n",
    "        clf = SVC(C=10, gamma=gamma).fit(X[train], y[train])\n",
    "        train_scores[i, j] = clf.score(X[train], y[train])\n",
    "        test_scores[i, j] = clf.score(X[test], y[test])\n",
    "        \n",
    "for i in range(n_iter):\n",
    "    pl.semilogx(gammas, train_scores[:, i], alpha=0.4, lw=2, c='b')\n",
    "    pl.semilogx(gammas, test_scores[:, i], alpha=0.4, lw=2, c='g')\n",
    "pl.ylabel(\"score for SVC(C=10, gamma=gamma)\")\n",
    "pl.xlabel(\"gamma\")\n",
    "pl.text(1e-6, 0.5, \"Underfitting\", fontsize=16, ha='center', va='bottom')\n",
    "pl.text(1e-4, 0.5, \"Good\", fontsize=16, ha='center', va='bottom')\n",
    "pl.text(1e-2, 0.5, \"Overfitting\", fontsize=16, ha='center', va='bottom')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decide C\n",
    "n_Cs = 10\n",
    "n_iter = 5\n",
    "cv = ShuffleSplit(n_samples, n_iter=n_iter, train_size=500, test_size=500,\n",
    "    random_state=0)\n",
    " \n",
    "train_scores = np.zeros((n_Cs, n_iter))\n",
    "test_scores = np.zeros((n_Cs, n_iter))\n",
    "Cs = np.logspace(-5, 5, n_Cs)\n",
    " \n",
    "for i, C in enumerate(Cs):\n",
    "    for j, (train, test) in enumerate(cv):\n",
    "        clf = SVC(C=C, gamma=1e-3).fit(X[train], y[train])\n",
    "        train_scores[i, j] = clf.score(X[train], y[train])\n",
    "        test_scores[i, j] = clf.score(X[test], y[test])\n",
    "        \n",
    "for i in range(n_iter):\n",
    "    pl.semilogx(Cs, train_scores[:, i], alpha=0.4, lw=2, c='b')\n",
    "    pl.semilogx(Cs, test_scores[:, i], alpha=0.4, lw=2, c='g')\n",
    "pl.ylabel(\"score for SVC(C=C, gamma=1e-3)\")\n",
    "pl.xlabel(\"C\")\n",
    "pl.text(1e-3, 0.5, \"Underfitting\", fontsize=16, ha='center', va='bottom')\n",
    "pl.text(1e3, 0.5, \"Few Overfitting\", fontsize=16, ha='center', va='bottom')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time \n",
    "a=time.clock()\n",
    "# 試切一份\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  #X放資料,y放類別,type:nparray \n",
    "print(\"train data shape: %r, train target shape: %r\"\n",
    "      % (X_train.shape, y_train.shape)) # train data shape: (1437, 64), train target shape: (1437,)\n",
    "print(\"test data shape: %r, test target shape: %r\"\n",
    "      % (X_test.shape, y_test.shape)) # test data shape: (360, 64), test target shape: (360,)\n",
    "svc = SVC(kernel='rbf', C=10, gamma=0.001).fit(X_train, y_train)\n",
    "\n",
    "# CV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "cv = ShuffleSplit(n_samples, n_iter=10, test_size=0.1,random_state=0)\n",
    "for cv_index, (train, test) in enumerate(cv):\n",
    "    print(\"# Cross Validation Iteration #%d\" % cv_index)\n",
    "    print(\"train indices: {0}...\".format(train[:10]))\n",
    "    print(\"test indices: {0}...\".format(test[:10]))\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc.score(X[train], y[train]), svc.score(X[test], y[test])))\n",
    "'''\n",
    "# 只會得到test score但速度快\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cv = ShuffleSplit(n_samples, n_iter=10, test_size=0.1, random_state=0)\n",
    " \n",
    "test_scores = cross_val_score(svc, X, y, cv=cv, n_jobs=2,scoring='roc_auc') # use default svc\n",
    "print test_scores\n",
    "'''\n",
    "\n",
    "# 算CV的mean,std\n",
    "from scipy.stats import sem\n",
    "def mean_score(scores):\n",
    "    \"\"\"Print the empirical mean score and standard error of the mean.\"\"\"\n",
    "    return (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores))\n",
    "print(mean_score(test_scores)) # Mean score: 0.489 (+/-0.023)\n",
    "time.clock()-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'gamma': 0.001} 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from pprint import pprint\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    " \n",
    "n_subsamples = 500\n",
    "X_small_train, y_small_train = X_train[:n_subsamples], y_train[:n_subsamples]\n",
    " \n",
    "gs_svc = GridSearchCV(SVC(), svc_params, cv=10, n_jobs=-1)\n",
    " \n",
    "gs_svc.fit(X_small_train, y_small_train)\n",
    " \n",
    "print gs_svc.best_params_, gs_svc.best_score_ # {'C': 10.0, 'gamma': 0.001} 0.982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n",
      "[[ 0.  0.]\n",
      " [ 1.  1.]]\n",
      "[[-1. -1.]\n",
      " [ 2.  2.]]\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(4, n_folds=2)\n",
    "for train, test in kf:\n",
    "    print(\"%s %s\" % (train, test))\n",
    "\n",
    "X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "print X_train\n",
    "print X_test\n",
    "print y[train]\n",
    "print y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 6 7 8 9] [0 4 5]\n",
      "[0 2 3 4 5 8 9] [1 6 7]\n",
      "[0 1 3 4 5 6 7 9] [2 8]\n",
      "[0 1 2 4 5 6 7 8] [3 9]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-5223218ea86a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "labels = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "skf = StratifiedKFold(labels, 4)\n",
    "for train, test in skf:\n",
    "    print(\"%s %s\" % (train, test))\n",
    "\n",
    "    \n",
    "X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
