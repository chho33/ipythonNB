{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將所有連結存成文字檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import re\n",
    "from datetime import date,datetime, timedelta \n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "import traceback\n",
    "import string  \n",
    "\n",
    "\n",
    "#################for db\n",
    "def connect(dbname,password,host,user,port=3306):\n",
    "    import MySQLdb\n",
    "    # 承德電腦 host=\"10.120.30.1\"\n",
    "    # localhost host = '127.0.0.1'\n",
    "    db = MySQLdb.connect(host=host,user=\"root\",passwd=password,db=dbname,charset=\"utf8\")\n",
    "    return db\n",
    "def insert_news(db,category=None,title=None,newsDate=None,newsTime=None,journalist=None,content=None,hit=None,url=None,source=\"ETToday\"):\n",
    "    cur = db.cursor()\n",
    "    #cur.execute(\"insert into news(class,title,newsDate,newsTime,journalist,content,hit,url,source) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\",[category,title,newsDate,newsTime,journalist,content,hit,url,source])\n",
    "    cur.execute(\"insert into news(class,title,newsDate,journalist,content,hit,url,source) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\",[category,title,newsDate,journalist,content,hit,url,source])\n",
    "    db.commit()\n",
    "def insert_comments(db,commentDate,commentTime,commenter,commenterUrl,comment,url,remark=None):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"insert into comments(commentDate,commentTime,commenter,commenterUrl,comment,url)values(%s,%s,%s,%s,%s,%s)\" ,[commentDate,commentTime,commenter,commenterUrl,comment,url])\n",
    "    db.commit()\n",
    "def insert_newsRaw(db,url,newsRaw):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"insert into newsRaw(`class`,url,newsRaw,source)values(%s,%s,%s,%s)\",[category,url,newsRaw,source])\n",
    "    db.commit()\n",
    "def get_newsRaw(db,url):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"select newsRaw from newsRaw where url = %s\",[url])\n",
    "    return cur.fetchone()\n",
    "def close(db):\n",
    "    db.close()\n",
    "#################for db\n",
    "\n",
    "\n",
    "#################Exception Handler\n",
    "def EH(breakpoint,newsDate=None,category=None,aLink=None):            \n",
    "    f=open(\"ETTodayException.log\",\"a\")\n",
    "    ExList=[\n",
    "        \"Log at: \" + str(datetime.today()) + \"\\n\",\n",
    "        \"\\t -News Date: \" + newsDate + \"\\n\",\n",
    "        \"\\t -News Category: \" + category + \"\\n\",\n",
    "        \"\\t -News url: \" + aLink + \"\\n\",\n",
    "        \"\\t -BreakPoint: \"+ breakpoint + \"\\n\",\n",
    "        \"\\t -Exception Traceback: \\n\", \n",
    "        str(traceback.format_exc()) + \"\\n\"\n",
    "    ]\n",
    "    f.writelines(ExList)\n",
    "    f.close\n",
    "#################Exception Handler\n",
    "\n",
    "#################get_all_pagelink\n",
    "sentinel = object()\n",
    "def get_all_pagelink(startday=(str(date.today()-timedelta(1))),endday=sentinel,category=[\"政治\",\"財經\",\"國際\",\"焦點\"],sleep_time=0):\n",
    "    \n",
    "    f=open(\"libtime.text\")\n",
    "    if endday is sentinel:\n",
    "        endday=startday\n",
    "    \n",
    "    def convert_day(d):\n",
    "        return datetime.strptime(d, '%Y%m%d')\n",
    "    \n",
    "    ed=convert_day(endday)\n",
    "    sd=convert_day(startday)\n",
    "\n",
    "    assert ed >= sd, \"endday must after startday\"\n",
    "    \n",
    "    ds = []\n",
    "    for i in xrange(0,(ed-sd).days+1):\n",
    "        ds.append(str((ed- timedelta(i)).strftime('%Y%m%d')))\n",
    "\n",
    "    url_all =[]\n",
    "    for newsDay in ds:\n",
    "        url_dic = {\"政治\":\"http://news.ltn.com.tw/newspaper/politics/%s\"%newsDay,\n",
    "                    \"財經\":\"http://news.ltn.com.tw/newspaper/business/%s\"%newsDay,\n",
    "                    \"國際\":\"http://news.ltn.com.tw/newspaper/world/%s\"%newsDay,\n",
    "                    \"社會\":\"http://news.ltn.com.tw/newspaper/society/%s\"%newsDay,\n",
    "                    \"焦點\":\"http://news.ltn.com.tw/newspaper/focus/%s\"%newsDay,\n",
    "                    \"地方\":\"http://news.ltn.com.tw/newspaper/local/%s\"%newsDay,\n",
    "                    \"娛樂\":\"http://news.ltn.com.tw/newspaper/entertainment/%s\"%newsDay,\n",
    "                   }\n",
    "        for cat in category:\n",
    "            try:\n",
    "                url_all.append(url_dic[str(cat)])\n",
    "            except:\n",
    "                print newsDay, cat, url_dic[str(cat)], \"\\n\"\n",
    "                print traceback.format_exc() + \"\\n\"\n",
    "    \n",
    "    aLink_all=[]\n",
    "    for cat in url_all:\n",
    "        try:\n",
    "            res = requests.get(cat)\n",
    "            res.encoding=\"utf-8\"\n",
    "            soup = BeautifulSoup(res.text)\n",
    "            p = soup.select('.tit')[0].text.encode('utf-8')\n",
    "            print type(p)\n",
    "            b=\"\".join(p.split(\"　\"))\n",
    "            c=\"\".join(b.split(\" \"))\n",
    "            page = re.search(\".*共有(\\d{1,4})筆.*\",c).group(1)\n",
    "            pageCount = int(math.ceil(float(page)/20))\n",
    "            soups = [soup]\n",
    "        except:\n",
    "            print cat+ \"\\n\"\n",
    "            print traceback.format_exc()+ \"\\n\"\n",
    "        if pageCount >1:\n",
    "            for i in xrange(2,pageCount+1):\n",
    "                curl = str(cat)+\"?page=%s\"%i\n",
    "                res = requests.get(curl)\n",
    "                res.encoding=\"utf-8\"\n",
    "                soups.append(BeautifulSoup(res.text))\n",
    "\n",
    "        for soup in soups:         \n",
    "            for block in soup.select(\".picword\"):\n",
    "                try:\n",
    "                    aLink='http://news.ltn.com.tw'+block['href']\n",
    "                    aLink_all.append(aLink)\n",
    "                    f.write(aLink+\"\\n\")\n",
    "                except:\n",
    "                    print cat\n",
    "                    raise\n",
    "                finally:\n",
    "                    f.close()\n",
    "        time.sleep(sleep_time)\n",
    "    f.close()\n",
    "    return aLink_all    \n",
    "#################get_all_pagelink\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寫進raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import re\n",
    "from datetime import date,datetime, timedelta \n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "import traceback\n",
    "import string  \n",
    "\n",
    "\n",
    "#################for db\n",
    "def connect(dbname,password,host,user,port=3306):\n",
    "    import MySQLdb\n",
    "    # 承德電腦 host=\"10.120.30.1\"\n",
    "    # localhost host = '127.0.0.1'\n",
    "    db = MySQLdb.connect(host=host,user=\"root\",passwd=password,db=dbname,charset=\"utf8\")\n",
    "    return db\n",
    "def insert_news(db,category=None,title=None,newsDate=None,newsTime=None,journalist=None,content=None,hit=None,url=None,source=\"ETToday\"):\n",
    "    cur = db.cursor()\n",
    "    #cur.execute(\"insert into news(class,title,newsDate,newsTime,journalist,content,hit,url,source) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\",[category,title,newsDate,newsTime,journalist,content,hit,url,source])\n",
    "    cur.execute(\"insert into news(class,title,newsDate,journalist,content,hit,url,source) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\",[category,title,newsDate,journalist,content,hit,url,source])\n",
    "    db.commit()\n",
    "def insert_comments(db,commentDate,commentTime,commenter,commenterUrl,comment,url,remark=None):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"insert into comments(commentDate,commentTime,commenter,commenterUrl,comment,url)values(%s,%s,%s,%s,%s,%s)\" ,[commentDate,commentTime,commenter,commenterUrl,comment,url])\n",
    "    db.commit()\n",
    "def insert_newsRaw(db,url,newsRaw):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"insert into newsRaw(`class`,url,newsRaw,source)values(%s,%s,%s,%s)\",[category,url,newsRaw,source])\n",
    "    db.commit()\n",
    "def get_newsRaw(db,url):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"select newsRaw from newsRaw where url = %s\",[url])\n",
    "    return cur.fetchone()\n",
    "def close(db):\n",
    "    db.close()\n",
    "#################for db\n",
    "\n",
    "\n",
    "#################Exception Handler\n",
    "def EH(breakpoint,newsDate=None,category=None,aLink=None):            \n",
    "    f=open(\"ETTodayException.log\",\"a\")\n",
    "    ExList=[\n",
    "        \"Log at: \" + str(datetime.today()) + \"\\n\",\n",
    "        \"\\t -News Date: \" + newsDate + \"\\n\",\n",
    "        \"\\t -News Category: \" + category + \"\\n\",\n",
    "        \"\\t -News url: \" + aLink + \"\\n\",\n",
    "        \"\\t -BreakPoint: \"+ breakpoint + \"\\n\",\n",
    "        \"\\t -Exception Traceback: \\n\", \n",
    "        str(traceback.format_exc()) + \"\\n\"\n",
    "    ]\n",
    "    f.writelines(ExList)\n",
    "    f.close\n",
    "#################Exception Handler\n",
    "\n",
    "#################crawl_all_rawpage_and_insert_into_db\n",
    "def all_rawpage(aLink_all,db=\"practice\",passwd=\"\",port=\"localhost\",user=\"root\"):\n",
    "    \n",
    "        #driver = webdriver.Firefox()\n",
    "        #driver = webdriver.Chrome()\n",
    "        driver = webdriver.PhantomJS()\n",
    "        webdriver.DesiredCapabilities.PHANTOMJS['phantomjs.page.customHeaders.Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "        webdriver.DesiredCapabilities.PHANTOMJS['phantomjs.page.customHeaders.Accept-Encoding'] = 'gzip, deflate, sdch'\n",
    "        webdriver.DesiredCapabilities.PHANTOMJS['phantomjs.page.customHeaders.Cache-Control'] = 'max-age=0'\n",
    "        webdriver.DesiredCapabilities.PHANTOMJS['phantomjs.page.customHeaders.Connection'] = 'keep-alive'\n",
    "        webdriver.DesiredCapabilities.PHANTOMJS['phantomjs.page.customHeaders.User-Agent'] = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36'\n",
    "    \n",
    "    for aLink in aLink_all:\n",
    "        #########rawpage breakpoint\n",
    "        try:\n",
    "            driver.get(aLink)\n",
    "            WebDriverWait(driver, 30).until(\n",
    "                    EC.visibility_of_element_located((By.ID, \"fbComments\"))\n",
    "                )\n",
    "            html = driver.page_source\n",
    "            cont = str(html.encode('utf-8'))\n",
    "            conn=connect(db,passwd,port,user)\n",
    "            insert_newsRaw(conn,aLink,cont,source=\"LibertyTime\")\n",
    "        except Exception as e:\n",
    "            EH('rawpage',newsDate=None,category=None,aLink=None) \n",
    "            print \"rawpage not write in DB \\n\", traceback.format_exc(),aLink,\"\\n\"\n",
    "            continue\n",
    "        finally:\n",
    "            driver.quit()\n",
    "            close(conn)\n",
    "        #########rawpage breakpoint                \n",
    "#################crawl_all_rawpage_and_insert_into_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寫進乾淨的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import re\n",
    "from datetime import date,datetime, timedelta \n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "import traceback\n",
    "import string  \n",
    "\n",
    "\n",
    "#################for db\n",
    "def connect(dbname,password,host,user,port=3306):\n",
    "    import MySQLdb\n",
    "    # 承德電腦 host=\"10.120.30.1\"\n",
    "    # localhost host = '127.0.0.1'\n",
    "    db = MySQLdb.connect(host=host,user=\"root\",passwd=password,db=dbname,charset=\"utf8\")\n",
    "    return db\n",
    "def insert_news(db,category=None,title=None,newsDate=None,newsTime=None,journalist=None,content=None,hit=None,url=None,source=\"ETToday\"):\n",
    "    cur = db.cursor()\n",
    "    #cur.execute(\"insert into news(class,title,newsDate,newsTime,journalist,content,hit,url,source) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\",[category,title,newsDate,newsTime,journalist,content,hit,url,source])\n",
    "    cur.execute(\"insert into news(class,title,newsDate,journalist,content,hit,url,source) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\",[category,title,newsDate,journalist,content,hit,url,source])\n",
    "    db.commit()\n",
    "def insert_comments(db,commentDate,commentTime,commenter,commenterUrl,comment,url,remark=None):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"insert into comments(commentDate,commentTime,commenter,commenterUrl,comment,url)values(%s,%s,%s,%s,%s,%s)\" ,[commentDate,commentTime,commenter,commenterUrl,comment,url])\n",
    "    db.commit()\n",
    "def insert_newsRaw(db,url,newsRaw):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"insert into newsRaw(`class`,url,newsRaw,source)values(%s,%s,%s,%s)\",[category,url,newsRaw,source])\n",
    "    db.commit()\n",
    "def get_newsRaw(db,url):\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"select newsRaw from newsRaw where url = %s\",[url])\n",
    "    return cur.fetchone()\n",
    "def close(db):\n",
    "    db.close()\n",
    "#################for db\n",
    "\n",
    "\n",
    "#################Exception Handler\n",
    "def EH(breakpoint,newsDate=None,category=None,aLink=None):            \n",
    "    f=open(\"ETTodayException.log\",\"a\")\n",
    "    ExList=[\n",
    "        \"Log at: \" + str(datetime.today()) + \"\\n\",\n",
    "        \"\\t -News Date: \" + newsDate + \"\\n\",\n",
    "        \"\\t -News Category: \" + category + \"\\n\",\n",
    "        \"\\t -News url: \" + aLink + \"\\n\",\n",
    "        \"\\t -BreakPoint: \"+ breakpoint + \"\\n\",\n",
    "        \"\\t -Exception Traceback: \\n\", \n",
    "        str(traceback.format_exc()) + \"\\n\"\n",
    "    ]\n",
    "    f.writelines(ExList)\n",
    "    f.close\n",
    "#################Exception Handler\n",
    "\n",
    "#################parse_page_and_insert_into_db\n",
    "hit = None\n",
    "sentinel = object()\n",
    "def parse_page(aLink_all,db=\"news\",passwd=\"\",port=\"localhost\",user=\"root\"):\n",
    "    #########retrieve_rawcontent_from_db\n",
    "    try:\n",
    "        conn=connect(db,passwd,port,user)\n",
    "        cur=conn.cursor()\n",
    "        cur.execute(\"select newsRaw,url from newsRaw;\")\n",
    "        allRaw=cur.fetchall()\n",
    "    except:\n",
    "        print \"retrieve rawcontent not succeed...\\n\", traceback.format_exc(),\"\\n\"\n",
    "    #########retrieve_rawcontent_from_db\n",
    "    \n",
    "    for raw in allRaw:\n",
    "        contRaw=raw[0]\n",
    "        soup=BeautifulSoup(contRaw)\n",
    "        aLink=raw[1]\n",
    "        content = soup.select('.content')[0]\n",
    "        newstext = content.select('#newstext')[0]\n",
    "        #########category breakpoint\n",
    "        try:\n",
    "            category=content.select('.guide a')[1].text.encode('utf-8')\n",
    "        except:\n",
    "            EH(breakpoint=\"category\",newsDate=newsDate,category=category,aLink=aLink)\n",
    "        #########category breakpoint\n",
    "\n",
    "        #########title breakpoint\n",
    "        try:\n",
    "            title=content.select('h1')[0].text.encode('utf-8')\n",
    "        except:\n",
    "            EH(breakpoint=\"title\",newsDate=newsDate,category=category,aLink=aLink)\n",
    "        #########title breakpoint\n",
    "\n",
    "        ########Date&Time breakpoint\n",
    "        try:\n",
    "            newsDate=content.select('#newstext span')[0].text.encode('utf-8')\n",
    "        #newsTime=timeRegex.group(3)\n",
    "        except:\n",
    "            EH(breakpoint=\"Date&Time\",newsDate=newsDate,category=category,aLink=aLink)\n",
    "        ########Date&Time breakpoint\n",
    "\n",
    "        #########content breakpoint\n",
    "        try:\n",
    "            \n",
    "            try:\n",
    "                for tag in newstext.select('script'):\n",
    "                    tag.decompose()\n",
    "                for tag in newstext.select('#newsad'):\n",
    "                    tag.decompose()                       \n",
    "            except:\n",
    "                pass \n",
    "            content=newstext.text\n",
    "        except:\n",
    "            EH(breakpoint=\"content\",newsDate=newsDate,category=category,aLink=aLink)\n",
    "        #########content breakpoint        \n",
    "\n",
    "        #########journist breakpoint\n",
    "        jor = ((newstext.select('p')[0].text.encode('utf-8')).split('／')[0].decode('utf-8'))\n",
    "        try:\n",
    "            journalist=re.search(u'.*(?:編譯|記者|◎)([^）]*)', jor).group(1).strip()\n",
    "        except:\n",
    "            EH(breakpoint='journalist',newsDate=newsDate,category=category,aLink=aLink) \n",
    "            print \"jor: \",aLink,\" WTF\"\n",
    "        #########journist breakpoint\n",
    "\n",
    "        '''#########comment breakpoint\n",
    "        comments=[]\n",
    "        try:\n",
    "            commLink0 = soupCont.find(\"div\",re.compile('fb-comments.*'))\n",
    "            commLink1= commLink0.select(' span > iframe')[0]['src']\n",
    "            commLink=re.sub(r\"numposts=\\d{1,3}&\", \"numposts=100&\", commLink1)\n",
    "            resComm = requests.get(commLink)\n",
    "            resComm.encoding = \"utf-8\"\n",
    "            soupComm = BeautifulSoup(resComm.text)      \n",
    "            commentBlock=soupComm.select('.postContainer.fsl.fwb.fcb')\n",
    "            for comm in commentBlock: \n",
    "                dt=re.search(\"(\\d\\d\\d\\d).(\\d{1,2}).(\\d{1,2}). (\\d{1,2}\\:\\d{1,2})\",comm.select('abbr')[0]['title'])\n",
    "                commentDate=\"%s-%02d-%02d\"%(dt.group(1),int(dt.group(2)),int(dt.group(3)))\n",
    "                commentTime=dt.group(4)\n",
    "                commenter=comm.select('.profileName')[0].text\n",
    "                commenterUrl=comm.select('.profileName')[0]['href']\n",
    "                comment=comm.select('.postText')[0].text\n",
    "                comments.append({'commentDate':commentDate,'commentTime':commentTime,'commenter':commenter,'commenterUrl':commenterUrl,'comment':comment,'url':aLink,'remark':None})\n",
    "        except Exception as e:\n",
    "            EH('comment',newsDate=None,category=None,aLink=None) \n",
    "        #########comment breakpoint'''\n",
    "\n",
    "        #################connect to db\n",
    "        #print category,title,newsDate,newsTime,journalist,content,hit,aLink,commentDate,commentTime,commenter,comment\n",
    "        try:\n",
    "            #insert_news(conn,category,title,newsDate,newsTime,journalist,content,hit,aLink)\n",
    "            insert_news(conn,category,title,newsDate,journalist,content,hit,aLink)\n",
    "            #for c in comments:\n",
    "            #    insert_comments(conn,c.values()[3],c.values()[6],c.values()[4],c.values()[1],c.values()[0],c.values()[5])\n",
    "        except:\n",
    "            print \"something not write in DB \\n\", traceback.format_exc(),aLink,\"\\n\"\n",
    "        finally:\n",
    "            close(conn)\n",
    "        #################connect to db\n",
    "#################parse_page_and_insert_into_db   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
