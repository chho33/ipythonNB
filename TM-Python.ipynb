{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ngram\n",
    "def ngram(sentence, n):\n",
    "    chunk = []\n",
    "    for i in xrange(0,len(sentence.decode('utf-8'))-n+1):\n",
    "        chunk.append(sentence.decode('utf-8')[i:i+n])\n",
    "    return chunk\n",
    "\n",
    "\n",
    "# segmentWord(try cut string in two segments)\n",
    "def segment(sentence):\n",
    "    n=len(sentence.decode('utf-8'))\n",
    "    segments=[]\n",
    "    for i in xrange(0,n):\n",
    "        seg1=sentence.decode('utf-8')[0:i]\n",
    "        seg2=sentence.decode('utf-8')[i:n+1]\n",
    "        segments.append([seg1,seg2])\n",
    "    return(segments)\n",
    "        \n",
    "    \n",
    "# word_freq\n",
    "def word_freq(sentences=None,n=6):     #senteces 為包含多個sentence的list\n",
    "    pieces=[]\n",
    "    for i in xrange(1,n+1):\n",
    "        for sentence in sentences:\n",
    "            pieces.append(ngram(sentence,i))\n",
    "    pieces_concat=[]\n",
    "    for piece in pieces:\n",
    "        for p in piece:\n",
    "            pieces_concat.append(p)\n",
    "    pieces_freq={}\n",
    "    for pf in pieces_concat:\n",
    "        pieces_freq[pf]=0\n",
    "    for pf in pieces_concat:\n",
    "        if pieces_freq[pf]>0:\n",
    "            pieces_freq[pf]+=1\n",
    "        else:\n",
    "            pieces_freq[pf]=1\n",
    "    return pieces_freq\n",
    "        \n",
    "\n",
    "# word_prob\n",
    "def word_prob(sentences=None,n=6):\n",
    "    pieces=[]\n",
    "    for i in xrange(1,n+1):\n",
    "        for sentence in sentences:\n",
    "            pieces.append(ngram(sentence,i))\n",
    "    pieces_concat=[]\n",
    "    for piece in pieces:\n",
    "        for p in piece:\n",
    "            pieces_concat.append(p)\n",
    "    pieces_freq={}\n",
    "    for pf in pieces_concat:\n",
    "        pieces_freq[pf]=0\n",
    "    for pf in pieces_concat:\n",
    "        if pieces_freq[pf]>0:\n",
    "            pieces_freq[pf]+=1\n",
    "        else:\n",
    "            pieces_freq[pf]=1\n",
    "    word_concat=\"\"\n",
    "    for sentence in sentences:\n",
    "        word_concat=word_concat+senctence\n",
    "    N=len(word_concat.decode('utf-8'))\n",
    "    pieces_prob={}\n",
    "    for pf in pieces_freq:\n",
    "        pieces_prob[pf]=pieces_freq[pf]/N\n",
    "    return pieces_prob\n",
    "\n",
    "    \n",
    "# cohesion  算每一個詞再斷詞後機率是否跟該詞獨立，一個字的會自動過濾掉\n",
    "def word_isolation(sentences,iso=0,n=6):  #iso表示想選擇isolation value >多少的數值\n",
    "    import math\n",
    "    word_group=word_freq(sentences,n)\n",
    "    word_concat=\"\"\n",
    "    for sentence in sentences:\n",
    "        word_concat=word_concat+sentence\n",
    "    N=len(word_concat.decode('utf-8'))\n",
    "    word_iso={}\n",
    "    for word in word_group:\n",
    "        word_mi=[]\n",
    "        wordseg=segment(str(word.encode('utf-8')))\n",
    "        for wseg in wordseg:\n",
    "            try:\n",
    "                word_f = word_group[word]\n",
    "                wseg1_f = word_group[wseg[0]]\n",
    "                wseg2_f = word_group[wseg[1]]\n",
    "                mi=math.log(N,2) + math.log(word_f,2) - math.log(wseg1_f,2) - math.log(wseg2_f,2)\n",
    "                word_mi.append(mi)\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            word_iso[word]=min(word_mi)\n",
    "        except:\n",
    "            pass\n",
    "    return dict((key,value) for key, value in word_iso.iteritems() if value >iso)\n",
    "\n",
    "\n",
    "\n",
    "# disorder\n",
    "def word_disorder(sentences,n=6,iso=0,d=1):    #val為isolation value, n為ngram長度, d為亂度閥值\n",
    "    #from itertools import chain\n",
    "    import re\n",
    "    import numpy\n",
    "    import math\n",
    "    pieces={}\n",
    "    for i in xrange(1,n+1):\n",
    "        c=[]\n",
    "        for sentence in sentences:\n",
    "            c=c+ngram(sentence,i)\n",
    "        pieces[i]=c\n",
    "\n",
    "    words_iso = word_isolation(sentences,iso,n)\n",
    "    words_freq = word_freq(sentences,n)\n",
    "    words_dis={}\n",
    "    i=0\n",
    "    for word in words_iso:\n",
    "        i+=1\n",
    "        try:\n",
    "            N=len(word)\n",
    "            if N<n:\n",
    "                base=pieces[N+1]\n",
    "\n",
    "                match1=[]\n",
    "                match2=[]\n",
    "                for b in base:\n",
    "\n",
    "                    if bool(re.search(\"^\"+word,b)):\n",
    "                        match1.append(b)\n",
    "                    if bool(re.search(word+\"$\",b)):\n",
    "                        match2.append(b)\n",
    "                freq1=[]\n",
    "                for m in set(match1):\n",
    "                    freq1.append(words_freq[m])\n",
    "\n",
    "                SUM=sum(freq1)\n",
    "                log_a=[-(math.log(float(a)/SUM,2)) for a in freq1]\n",
    "                pre=numpy.mean(log_a)\n",
    "\n",
    "                freq2=[]\n",
    "                for m in set(match2):\n",
    "                    freq2.append(words_freq[m])\n",
    "\n",
    "                SUM2=sum(freq2)\n",
    "                log_a2=[-(math.log(float(a)/SUM,2)) for a in freq2]\n",
    "                post=numpy.mean(log_a2)\n",
    "\n",
    "                if math.isnan(pre)|math.isnan(post):\n",
    "                    if math.isnan(pre):\n",
    "                        words_dis[word]=post\n",
    "                    else:\n",
    "                        words_dis[word]=pre\n",
    "                else:\n",
    "                    words_dis[word]=min([pre,post]) \n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    return dict((key,value) for key, value in words_dis.iteritems() if value >d)\n",
    "\n",
    "\n",
    "def remove_punc(sentences):\n",
    "    import string \n",
    "    import re\n",
    "    delEStr = '['+string.punctuation + u' '+']'\n",
    "    delCStr = u'[▲►★《》（）()&%￥#@$！?,.><*{}\\'`\"【】　、。，：；”～‘＠＃＄％︿＆＊｀＝－＋｜／＼？『』」「\\/]'\n",
    "    s=[]\n",
    "    for sentence in sentences:\n",
    "        if type(sentence)==unicode:\n",
    "            sentence=re.sub(delEStr,u'',sentence)\n",
    "            sentence=re.sub(delCStr,u'',sentence)\n",
    "            s.append(sentence.encode('utf-8'))\n",
    "        else:\n",
    "            sentence=re.sub(delEStr,u'',sentence.decode('utf-8'))\n",
    "            sentence=re.sub(delCStr,u'',sentence)\n",
    "            s.append(sentence.encode('utf-8'))\n",
    "    return s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "當局 1.58496250072\n",
      "達志 1.08496250072\n",
      "可以 1.20751874964\n",
      "在男 1.08496250072\n",
      "選擇要 1.08496250072\n",
      "可以自 1.08496250072\n",
      "男或女監 1.08496250072\n",
      "未來 1.08496250072\n",
      "變性 2.0\n",
      "================\n",
      "中心綜合報導 8.49984588708\n",
      "使得當局開始 8.49984588708\n",
      "暴該起事件 8.49984588708\n",
      "組織表示這個 8.49984588708\n",
      "綜合報導今年 8.49984588708\n",
      "個決定顯 8.49984588708\n",
      "導今年 8.49984588708\n",
      "何網 8.49984588708\n",
      "意圖國際中心 8.49984588708\n",
      "事件 8.49984588708\n",
      "個決 8.49984588708\n",
      "留長頭 8.49984588708\n",
      "莉納Ve 8.49984588708\n",
      "件使 8.49984588708\n",
      "lina被 8.49984588708\n",
      "是其他 8.49984588708\n",
      "意圖國際 8.49984588708\n",
      "宣布 8.49984588708\n",
      "粉絲 8.49984588708\n",
      "別人士的尊重 8.49984588708\n",
      "day 8.49984588708\n",
      "年4月 8.49984588708\n",
      "納Ve 8.49984588708\n",
      "其他 8.49984588708\n",
      "心綜合報導 8.49984588708\n",
      "電視台未經 8.49984588708\n",
      "夠留 8.49984588708\n",
      "中心綜合 8.49984588708\n",
      "傳送第一手 8.49984588708\n",
      "其他囚犯襲 8.49984588708\n",
      "但為了避 8.49984588708\n",
      "剃光 8.49984588708\n",
      "他囚犯襲 8.49984588708\n",
      "lina被揭 8.49984588708\n",
      "專用任何網 8.49984588708\n",
      "網站 8.49984588708\n",
      "國際中心 8.49984588708\n",
      "專用任何 8.49984588708\n",
      "尊重 8.49984588708\n",
      "ronic 8.49984588708\n",
      "站報刊電 8.49984588708\n",
      "納Ver 8.49984588708\n",
      "莉納Ver 8.49984588708\n",
      "至是其 8.49984588708\n",
      "將她的長髮剃 8.49984588708\n",
      "他囚犯襲擊 8.49984588708\n",
      "雲專 8.49984588708\n",
      "聖保 8.49984588708\n",
      "重傳送第 8.49984588708\n",
      "導今 8.49984588708\n",
      "y東森 8.49984588708\n",
      "任何網站報刊 8.49984588708\n",
      "起事 8.49984588708\n",
      "褲更能夠 8.49984588708\n",
      "年4 8.49984588708\n",
      "肯定當地 8.49984588708\n",
      "何網站報刊 8.49984588708\n",
      "何網站 8.49984588708\n",
      "即時粉絲團 8.49984588708\n",
      "心綜合 8.49984588708\n",
      "暴該起 8.49984588708\n",
      "意圖國際中 8.49984588708\n",
      "鎖定ET即時 8.49984588708\n",
      "day東森 8.49984588708\n",
      "際中 8.49984588708\n",
      "羅變性女囚波 8.49984588708\n",
      "分或全部轉 8.49984588708\n",
      "始正 8.49984588708\n",
      "和內 8.49984588708\n",
      "和內衣褲 8.49984588708\n",
      "caBol 8.49984588708\n",
      "心綜 8.49984588708\n",
      "仍只 8.49984588708\n",
      "爐立 8.49984588708\n",
      "月巴西聖保 8.49984588708\n",
      "LGBT組 8.49984588708\n",
      "版權照 8.49984588708\n",
      "措施一出爐立 8.49984588708\n",
      "重傳 8.49984588708\n",
      "手的新聞鎖 8.49984588708\n",
      "任何網 8.49984588708\n",
      "er 8.49984588708\n",
      "波莉納Ve 8.49984588708\n",
      "月巴西聖 8.49984588708\n",
      "森新聞雲專用 8.49984588708\n",
      "跨性別 8.49984588708\n",
      "森新聞雲專 8.49984588708\n",
      "剃光扯 8.49984588708\n",
      "被揭 8.49984588708\n",
      "國際中心綜合 8.49984588708\n",
      "納V 8.49984588708\n",
      "專用任何網站 8.49984588708\n",
      "合報導 8.49984588708\n",
      "各方 8.49984588708\n",
      "是其 8.49984588708\n",
      "用任何 8.49984588708\n",
      "重傳送 8.49984588708\n",
      "與跨 8.49984588708\n",
      "時粉絲團 8.49984588708\n",
      "其他囚犯襲擊 8.49984588708\n",
      "第一手 8.49984588708\n",
      "森新聞雲 8.49984588708\n",
      "織表示這個 8.49984588708\n",
      "國際中心綜 8.49984588708\n",
      "員警甚至 8.49984588708\n",
      "導今年4 8.49984588708\n",
      "絲團 8.49984588708\n",
      "時粉 8.49984588708\n",
      "褲更能夠留 8.49984588708\n",
      "地LG 8.49984588708\n",
      "經達志影像許 8.49984588708\n",
      "eronic 8.49984588708\n",
      "內衣褲更能夠 8.49984588708\n",
      "雲專用任何 8.49984588708\n",
      "即時 8.49984588708\n",
      "網站報刊 8.49984588708\n",
      "夠留長頭髮但 8.49984588708\n",
      "刊電視台 8.49984588708\n",
      "用任何網 8.49984588708\n",
      "網站報刊電 8.49984588708\n",
      "國際 8.49984588708\n",
      "全部轉載 8.49984588708\n",
      "員警甚至是其 8.49984588708\n",
      "傳送第 8.49984588708\n",
      "GBT組 8.49984588708\n",
      "用任何網站 8.49984588708\n",
      "卒員警甚至 8.49984588708\n",
      "襲擊不僅將 8.49984588708\n",
      "光扯 8.49984588708\n",
      "許可不得部分 8.49984588708\n",
      "各方的肯 8.49984588708\n",
      "刻受到來自各 8.49984588708\n",
      "即時粉絲 8.49984588708\n",
      "即時粉絲團就 8.49984588708\n",
      "用任 8.49984588708\n",
      "警甚至是其他 8.49984588708\n",
      "today東 8.49984588708\n",
      "員警 8.49984588708\n",
      "警甚至 8.49984588708\n",
      "尊重傳 8.49984588708\n",
      "尊重傳送第 8.49984588708\n",
      "甚至是其他 8.49984588708\n",
      "地LGBT組 8.49984588708\n",
      "波莉納V 8.49984588708\n",
      "Ver 8.49984588708\n",
      "東森新聞雲 8.49984588708\n",
      "卒員警甚至是 8.49984588708\n",
      "襲擊 8.49984588708\n",
      "方的肯定當地 8.49984588708\n",
      "甚至是 8.49984588708\n",
      "肯定當地LG 8.49984588708\n",
      "綜合報導 8.49984588708\n",
      "別人士的尊 8.49984588708\n",
      "留長頭髮但 8.49984588708\n",
      "肯定當地L 8.49984588708\n",
      "至是其他 8.49984588708\n",
      "決定顯 8.49984588708\n",
      "供ETt 8.49984588708\n",
      "專用 8.49984588708\n",
      "至是 8.49984588708\n",
      "綜合 8.49984588708\n",
      "措施一出爐 8.49984588708\n",
      "全部轉 8.49984588708\n",
      "today 8.49984588708\n",
      "甚至 8.49984588708\n",
      "中心綜 8.49984588708\n",
      "雲專用任何網 8.49984588708\n",
      "供ETtod 8.49984588708\n",
      "電視台 8.49984588708\n",
      "但為了避免 8.49984588708\n",
      "今年4 8.49984588708\n",
      "員警甚至是 8.49984588708\n",
      "站報刊電視台 8.49984588708\n",
      "益於29日推 8.49984588708\n",
      "今年4月 8.49984588708\n",
      "合報導今年4 8.49984588708\n",
      "莉納 8.49984588708\n",
      "導今年4月 8.49984588708\n",
      "員警甚 8.49984588708\n",
      "意圖國 8.49984588708\n",
      "y東 8.49984588708\n",
      "傳送 8.49984588708\n",
      "聖保羅 8.49984588708\n",
      "時粉絲 8.49984588708\n",
      "何網站報刊電 8.49984588708\n",
      "際中心綜合 8.49984588708\n",
      "刊電 8.49984588708\n",
      "專用任 8.49984588708\n",
      "雲專用任 8.49984588708\n",
      "站報刊 8.49984588708\n",
      "綜合報導今 8.49984588708\n",
      "際中心 8.49984588708\n",
      "tod 8.49984588708\n",
      "卒員警甚 8.49984588708\n",
      "事件使 8.49984588708\n",
      "團就 8.49984588708\n",
      "頭髮但 8.49984588708\n",
      "國際中 8.49984588708\n",
      "4月 8.49984588708\n",
      "中心 8.49984588708\n",
      "任何網站 8.49984588708\n",
      "爐立刻 8.49984588708\n",
      "織表示這個決 8.49984588708\n",
      "合報導今年 8.49984588708\n",
      "起事件使 8.49984588708\n",
      "轉載 8.49984588708\n",
      "合報導今 8.49984588708\n",
      "是其他囚犯襲 8.49984588708\n",
      "卒員 8.49984588708\n",
      "立刻 8.49984588708\n",
      "警甚至是其 8.49984588708\n",
      "鎖定ET即 8.49984588708\n",
      "粉絲團就 8.49984588708\n",
      "組織 8.49984588708\n",
      "任何 8.49984588708\n",
      "粉絲團 8.49984588708\n",
      "LG 8.49984588708\n",
      "尊重傳送 8.49984588708\n",
      "刊電視台未經 8.49984588708\n",
      "送第一手 8.49984588708\n",
      "推出新規 8.49984588708\n",
      "警甚 8.49984588708\n",
      "波莉 8.49984588708\n",
      "雲專用 8.49984588708\n",
      "4月巴西聖保 8.49984588708\n",
      "GBT組織 8.49984588708\n",
      "起事件 8.49984588708\n",
      "時粉絲團就 8.49984588708\n",
      "際中心綜 8.49984588708\n",
      "y東森新聞雲 8.49984588708\n",
      "分或全部轉載 8.49984588708\n",
      "波莉納Ver 8.49984588708\n",
      "波莉納 8.49984588708\n",
      "月巴西聖保羅 8.49984588708\n",
      "方的肯 8.49984588708\n",
      "警甚至是 8.49984588708\n",
      "東森新聞雲專 8.49984588708\n",
      "雙性戀與跨 8.49984588708\n",
      "雙性戀與 8.49984588708\n",
      "保羅 8.49984588708\n",
      "與跨性別 8.49984588708\n",
      "莉納V 8.49984588708\n",
      "絲團就 8.49984588708\n",
      "心綜合報導今 8.49984588708\n",
      "即時粉 8.49984588708\n",
      "地L 8.49984588708\n",
      "卒員警 8.49984588708\n",
      "LGBT組織 8.49984588708\n",
      "Ve 8.49984588708\n",
      "4月巴西聖 8.49984588708\n",
      "分或全 8.49984588708\n",
      "年4月巴西聖 8.49984588708\n",
      "送第 8.49984588708\n",
      "day東 8.49984588708\n",
      "今年 8.49984588708\n",
      "避免 8.49984588708\n",
      "內衣褲 8.49984588708\n",
      "台未經 8.49984588708\n",
      "頭髮但為了避 8.49984588708\n",
      "擊不僅將 8.49984588708\n",
      "暴該起事 8.49984588708\n",
      "甚至是其 8.49984588708\n",
      "暴該起事件使 8.49984588708\n",
      "夠留長頭 8.49984588708\n",
      "重傳送第一手 8.49984588708\n",
      "東森 8.49984588708\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "text=['''▲巴西當局於29日宣布，未來變性受刑人可以自由選擇要到男或女監獄服刑。（圖／達志示意圖）國際中心／綜合報導　今年4月巴西聖保羅變性女囚波莉納（Veronica Bolina）被揭穿多次遭男性獄卒、員警甚至是其他囚犯襲擊，不僅將她的長髮剃光、扯開她的衣服更多次遭到施暴。該起事件使得當局開始正視變性囚犯的權益，於29日推出新規定，表示這些人士可以自行選擇要在男或女監獄服刑。變性人未來不僅可以自行選擇要在男或女監服刑，也可以穿女性囚服和內衣褲，更能夠留長頭髮；但為了避免女同志在男監遭到侵犯，這些人未來仍只能在女監服刑。該措施一出爐立刻受到來自各方的肯定，當地LGBT組織表示這個決定顯示當局對男女同性戀、雙性戀與跨性別人士的尊重。►►►傳送第一手的新聞，鎖定《ET即時》粉絲團就對了！★圖片為版權照片，由達志影像供《ETtoday東森新聞雲》專用，任何網站、報刊、電視台未經達志影像許可，不得部分或全部轉載！''']\n",
    "#text=[\"巴西當局於29日宣布，未來變性受刑人可以自由選擇要到男或女監獄服刑巴西我巴西可以巴西可巴西\"]\n",
    "text=remove_punc(text)\n",
    "\n",
    "dis=word_disorder(text,n=6,iso=1,d=1)\n",
    "\n",
    "for w in dis:\n",
    "    print w, dis[w]\n",
    "\n",
    "print \"======================\"\n",
    "    \n",
    "coh=word_isolation(text,iso=8)\n",
    "\n",
    "for c in coh:\n",
    "    print c, coh[c]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# disorder archive\n",
    "def word_disorder(sentences,val=10,n=6):    #val為isolation value, n為ngram\n",
    "    from itertools import chain\n",
    "    import re\n",
    "    import numpy\n",
    "    \n",
    "    pieces={}\n",
    "    for i in xrange(1,n+1):\n",
    "        c=[]\n",
    "        for sentence in sentences:\n",
    "            #c = chain.from_iterable([c,ngram(sentence,i)])\n",
    "            c=c+ngram(sentence,i)\n",
    "        pieces[i]=c\n",
    "    words_iso = word_isolation(sentences,val,n)\n",
    "    \n",
    "    words_freq = word_freq(sentences,n)\n",
    "    words_dis={}\n",
    "    for i in xrange(1,n):\n",
    "        words_set=dict((key,value) for key, value in words_freq.iteritems() if len(key) == i)\n",
    "        for word in words_set:\n",
    "            if i<n:\n",
    "                base=pieces[i+1]\n",
    "                match1=[]\n",
    "                for b in base:\n",
    "                    if bool(re.search(\"^\"+word,b)):\n",
    "                        match1.append(b)\n",
    "                freq1=[]\n",
    "                for m in match1:\n",
    "                    freq1.append(words_freq[m])\n",
    "                SUM=sum(freq1)\n",
    "                log_a=[-(math.log(float(a)/SUM,2)) for a in freq1]\n",
    "                pre=numpy.mean(log_a)\n",
    "\n",
    "                match2=[]\n",
    "                for b in base:\n",
    "                    if bool(re.search(word+\"$\",b)):\n",
    "                        match2.append(b)\n",
    "                freq2=[]\n",
    "                for m in match2:\n",
    "                    freq2.append(words_freq[m])\n",
    "                SUM2=sum(freq2)\n",
    "                log_a2=[-(math.log(float(a)/SUM,2)) for a in freq2]\n",
    "                post=numpy.mean(log_a2)\n",
    "\n",
    "                if math.isnan(pre)|math.isnan(post):\n",
    "                    if math.isnan(pre):\n",
    "                        words_dis[word]=post\n",
    "                    else:\n",
    "                         words_dis[word]=pre\n",
    "                else:\n",
    "                    words_dis[word]=min([pre,post])    \n",
    "            else:\n",
    "                pass\n",
    "    return words_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test disorder\n",
    "import re\n",
    "import numpy\n",
    "import math\n",
    "val=10\n",
    "n=6\n",
    "sentences=text\n",
    "pieces={}\n",
    "for i in xrange(1,n+1):\n",
    "    c=[]\n",
    "    for sentence in sentences:\n",
    "        #c = chain.from_iterable([c,ngram(sentence,i)])\n",
    "        c=c+ngram(sentence,i)\n",
    "    pieces[i]=c\n",
    "    #for p in pieces[i]:\n",
    "    #    print p\n",
    "    #print \"----------\"\n",
    "words_iso = word_isolation(sentences,val,n)\n",
    "\n",
    "words_freq = word_freq(sentences,n)\n",
    "#for f in words_freq:\n",
    "#    print f\n",
    "words_dis={}\n",
    "print len(words_freq)\n",
    "i=0\n",
    "for word in words_freq:\n",
    "    i+=1\n",
    "    print \"no.\" + str(i)\n",
    "    try:\n",
    "        N=len(word)\n",
    "        if N<n:\n",
    "            base=pieces[N+1]\n",
    "            print \"字長度：\"+str(len(word))\n",
    "            print \"要比較的字:\" , word, \"^\"+word, word+\"$\"\n",
    "            print \"被比較的字:\"\n",
    "            match1=[]\n",
    "            match2=[]\n",
    "            for b in base:\n",
    "                print \"\\t\", b\n",
    "                print \"\\t\",\"  比較:\",\"^\"+word, bool(re.search(\"^\"+word,b))\n",
    "                print \"\\t\",\"  比較:\",\"^\"+word, bool(re.search(word+\"$\",b))\n",
    "                #print \"-------\"\n",
    "                if bool(re.search(\"^\"+word,b)):\n",
    "                    match1.append(b)\n",
    "                if bool(re.search(word+\"$\",b)):\n",
    "                    match2.append(b)\n",
    "            '''print \"match-pre: \"\n",
    "            for m in match1:\n",
    "                print m\n",
    "            print \"match-post: \"\n",
    "            for m in match2:\n",
    "                print m'''\n",
    "                \n",
    "            \n",
    "            freq1=[]\n",
    "            for m in set(match1):\n",
    "                print \"match-pre:\",m,words_freq[m]\n",
    "                freq1.append(words_freq[m])\n",
    "            print \"freq-pre:\",freq1\n",
    "            print \"match1\", match1\n",
    "            \n",
    "            SUM=sum(freq1)\n",
    "            print \"SUM-pre:\",SUM\n",
    "            log_a=[-(math.log(float(a)/SUM,2)) for a in freq1]\n",
    "            print \"log-pre:\",log_a\n",
    "            pre=numpy.mean(log_a)\n",
    "            print \"mean-pre:\",pre\n",
    "\n",
    "            freq2=[]\n",
    "            for m in set(match2):\n",
    "                print \"match-post:\",m,words_freq[m]\n",
    "                freq2.append(words_freq[m])\n",
    "            print \"freq-post:\",freq2\n",
    "\n",
    "            SUM2=sum(freq2)\n",
    "            print \"SUM-post:\",SUM2\n",
    "            log_a2=[-(math.log(float(a)/SUM,2)) for a in freq2]\n",
    "            print \"log-post:\",log_a2\n",
    "            post=numpy.mean(log_a2)\n",
    "            print \"mean-post:\", post\n",
    "            #mean(-log2(freq1/sum(freq1)))\n",
    "\n",
    "            if math.isnan(pre)|math.isnan(post):\n",
    "                if math.isnan(pre):\n",
    "                    words_dis[word]=post\n",
    "                    print \"亂度:\", words_dis[word]\n",
    "                else:\n",
    "                    words_dis[word]=pre\n",
    "                    print \"亂度:\", words_dis[word]         \n",
    "            else:\n",
    "                words_dis[word]=min([pre,post]) \n",
    "                print \"亂度:\", words_dis[word]\n",
    "        else:\n",
    "            print '字長度:',len(word)\n",
    "            pass\n",
    "        print \"========\"\n",
    "    except:\n",
    "        print \"error\"\n",
    "        print \"========\"\n",
    "        pass\n",
    "print words_dis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
