{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ngram\n",
    "def ngram(sentence, n):\n",
    "    chunk = []\n",
    "    for i in xrange(0,len(sentence.decode('utf-8'))-n+1):\n",
    "        chunk.append(sentence.decode('utf-8')[i:i+n])\n",
    "    return chunk\n",
    "\n",
    "\n",
    "# segmentWord(try cut string in two segments)\n",
    "def segment(sentence):\n",
    "    n=len(sentence.decode('utf-8'))\n",
    "    segments=[]\n",
    "    for i in xrange(0,n):\n",
    "        seg1=sentence.decode('utf-8')[0:i]\n",
    "        seg2=sentence.decode('utf-8')[i:n+1]\n",
    "        segments.append([seg1,seg2])\n",
    "    return(segments)\n",
    "        \n",
    "    \n",
    "# word_freq\n",
    "def word_freq(sentences=None,n=6):     #senteces 為包含多個sentence的list\n",
    "    pieces=[]\n",
    "    for i in xrange(1,n+1):\n",
    "        for sentence in sentences:\n",
    "            pieces.append(ngram(sentence,i))\n",
    "    pieces_concat=[]\n",
    "    for piece in pieces:\n",
    "        for p in piece:\n",
    "            pieces_concat.append(p)\n",
    "    pieces_freq={}\n",
    "    for pf in pieces_concat:\n",
    "        pieces_freq[pf]=0\n",
    "    for pf in pieces_concat:\n",
    "        if pieces_freq[pf]>0:\n",
    "            pieces_freq[pf]+=1\n",
    "        else:\n",
    "            pieces_freq[pf]=1\n",
    "    return pieces_freq\n",
    "        \n",
    "\n",
    "# word_prob\n",
    "def word_prob(sentences=None,n=6):\n",
    "    pieces=[]\n",
    "    for i in xrange(1,n+1):\n",
    "        for sentence in sentences:\n",
    "            pieces.append(ngram(sentence,i))\n",
    "    pieces_concat=[]\n",
    "    for piece in pieces:\n",
    "        for p in piece:\n",
    "            pieces_concat.append(p)\n",
    "    pieces_freq={}\n",
    "    for pf in pieces_concat:\n",
    "        pieces_freq[pf]=0\n",
    "    for pf in pieces_concat:\n",
    "        if pieces_freq[pf]>0:\n",
    "            pieces_freq[pf]+=1\n",
    "        else:\n",
    "            pieces_freq[pf]=1\n",
    "    word_concat=\"\"\n",
    "    for sentence in sentences:\n",
    "        word_concat=word_concat+senctence\n",
    "    N=len(word_concat.decode('utf-8'))\n",
    "    pieces_prob={}\n",
    "    for pf in pieces_freq:\n",
    "        pieces_prob[pf]=pieces_freq[pf]/N\n",
    "    return pieces_prob\n",
    "\n",
    "    \n",
    "# cohesion  算每一個詞再斷詞後機率是否跟該詞獨立，一個字的會自動過濾掉\n",
    "def word_isolation(sentences,iso=0,n=6):  #iso表示想選擇isolation value >多少的數值\n",
    "    import math\n",
    "    word_group=word_freq(sentences,n)\n",
    "    word_concat=\"\"\n",
    "    for sentence in sentences:\n",
    "        word_concat=word_concat+sentence\n",
    "    N=len(word_concat.decode('utf-8'))\n",
    "    word_iso={}\n",
    "    for word in word_group:\n",
    "        word_mi=[]\n",
    "        wordseg=segment(str(word.encode('utf-8')))\n",
    "        for wseg in wordseg:\n",
    "            try:\n",
    "                word_f = word_group[word]\n",
    "                wseg1_f = word_group[wseg[0]]\n",
    "                wseg2_f = word_group[wseg[1]]\n",
    "                mi=math.log(N,2) + math.log(word_f,2) - math.log(wseg1_f,2) - math.log(wseg2_f,2)\n",
    "                word_mi.append(mi)\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            word_iso[word]=min(word_mi)\n",
    "        except:\n",
    "            pass\n",
    "    return dict((key,value) for key, value in word_iso.iteritems() if value >=iso)\n",
    "\n",
    "\n",
    "\n",
    "# disorder\n",
    "def word_disorder(sentences,n=6,iso=0,d=1):    #val為isolation value, n為ngram長度, d為亂度閥值\n",
    "    #from itertools import chain\n",
    "    import re\n",
    "    import numpy\n",
    "    import math\n",
    "    pieces={}\n",
    "    for i in xrange(1,n+1):\n",
    "        c=[]\n",
    "        for sentence in sentences:\n",
    "            c=c+ngram(sentence,i)\n",
    "        pieces[i]=c\n",
    "\n",
    "    words_iso = word_isolation(sentences,iso,n)\n",
    "    words_freq = word_freq(sentences,n)\n",
    "    words_dis={}\n",
    "    i=0\n",
    "    for word in words_iso:\n",
    "        i+=1\n",
    "        try:\n",
    "            N=len(word)\n",
    "            if N<n:\n",
    "                base=pieces[N+1]\n",
    "\n",
    "                match1=[]\n",
    "                match2=[]\n",
    "                for b in base:\n",
    "\n",
    "                    if bool(re.search(\"^\"+word,b)):\n",
    "                        match1.append(b)\n",
    "                    if bool(re.search(word+\"$\",b)):\n",
    "                        match2.append(b)\n",
    "                freq1=[]\n",
    "                for m in set(match1):\n",
    "                    freq1.append(words_freq[m])\n",
    "\n",
    "                SUM=sum(freq1)\n",
    "                log_a=[-(math.log(float(a)/SUM,2)) for a in freq1]\n",
    "                pre=numpy.mean(log_a)\n",
    "\n",
    "                freq2=[]\n",
    "                for m in set(match2):\n",
    "                    freq2.append(words_freq[m])\n",
    "\n",
    "                SUM2=sum(freq2)\n",
    "                log_a2=[-(math.log(float(a)/SUM,2)) for a in freq2]\n",
    "                post=numpy.mean(log_a2)\n",
    "\n",
    "                if math.isnan(pre)|math.isnan(post):\n",
    "                    if math.isnan(pre):\n",
    "                        words_dis[word]=post\n",
    "                    else:\n",
    "                        words_dis[word]=pre\n",
    "                else:\n",
    "                    words_dis[word]=min([pre,post]) \n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    return dict((key,value) for key, value in words_dis.iteritems() if value >=d)\n",
    "\n",
    "\n",
    "def remove_punc(sentences):\n",
    "    import string \n",
    "    import re\n",
    "    delEStr = '['+string.punctuation + u' '+']'\n",
    "    delCStr = u'[▲►★《》（）()&%￥#@$！?,.><*{}\\'`\"【】　、。，：；”～‘＠＃＄％︿＆＊｀＝－＋｜／＼？『』」「\\/]'\n",
    "    s=[]\n",
    "    for sentence in sentences:\n",
    "        if type(sentence)==unicode:\n",
    "            sentence=re.sub(delEStr,u'',sentence)\n",
    "            sentence=re.sub(delCStr,u'',sentence)\n",
    "            s.append(sentence.encode('utf-8'))\n",
    "        else:\n",
    "            sentence=re.sub(delEStr,u'',sentence.decode('utf-8'))\n",
    "            sentence=re.sub(delCStr,u'',sentence)\n",
    "            s.append(sentence.encode('utf-8'))\n",
    "    return s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這些人 1.0\n",
      "女同 1.0\n",
      "男或女監 1.08496250072\n",
      "當局 1.58496250072\n",
      "表示這 1.0\n",
      "於29日 1.0\n",
      "巴西 1.0\n",
      "囚犯 1.0\n",
      "達志 1.08496250072\n",
      "她的 1.0\n",
      "遭到 1.0\n",
      "性戀 1.0\n",
      "變性 2.0\n",
      "不僅 1.0\n",
      "人士 1.0\n",
      "新聞 1.0\n",
      "在男 1.08496250072\n",
      "選擇要 1.08496250072\n",
      "可以自 1.08496250072\n",
      "ET 1.0\n",
      "達志影像 1.0\n",
      "人未來 1.0\n",
      "多次遭 1.0\n",
      "可以 1.20751874964\n",
      "性囚 1.0\n",
      "服刑 1.0\n",
      "女監 1.0\n",
      "未來 1.08496250072\n",
      "女監服刑 1.0\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "text=['''▲巴西當局於29日宣布，未來變性受刑人可以自由選擇要到男或女監獄服刑。（圖／達志示意圖）國際中心／綜合報導　今年4月巴西聖保羅變性女囚波莉納（Veronica Bolina）被揭穿多次遭男性獄卒、員警甚至是其他囚犯襲擊，不僅將她的長髮剃光、扯開她的衣服更多次遭到施暴。該起事件使得當局開始正視變性囚犯的權益，於29日推出新規定，表示這些人士可以自行選擇要在男或女監獄服刑。變性人未來不僅可以自行選擇要在男或女監服刑，也可以穿女性囚服和內衣褲，更能夠留長頭髮；但為了避免女同志在男監遭到侵犯，這些人未來仍只能在女監服刑。該措施一出爐立刻受到來自各方的肯定，當地LGBT組織表示這個決定顯示當局對男女同性戀、雙性戀與跨性別人士的尊重。►►►傳送第一手的新聞，鎖定《ET即時》粉絲團就對了！★圖片為版權照片，由達志影像供《ETtoday東森新聞雲》專用，任何網站、報刊、電視台未經達志影像許可，不得部分或全部轉載！''']\n",
    "#text=[\"巴西當局於29日宣布，未來變性受刑人可以自由選擇要到男或女監獄服刑巴西我巴西可以巴西可巴西\"]\n",
    "text=remove_punc(text)\n",
    "\n",
    "dis=word_disorder(text,n=6,iso=1,d=1)\n",
    "\n",
    "for w in dis:\n",
    "    print w, dis[w]\n",
    "\n",
    "print \"======================\"\n",
    "    \n",
    "#coh=word_isolation(text,iso=9)\n",
    "\n",
    "#for c in coh:\n",
    "#    print c, coh[c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# disorder archive\n",
    "def word_disorder(sentences,val=10,n=6):    #val為isolation value, n為ngram\n",
    "    from itertools import chain\n",
    "    import re\n",
    "    import numpy\n",
    "    \n",
    "    pieces={}\n",
    "    for i in xrange(1,n+1):\n",
    "        c=[]\n",
    "        for sentence in sentences:\n",
    "            #c = chain.from_iterable([c,ngram(sentence,i)])\n",
    "            c=c+ngram(sentence,i)\n",
    "        pieces[i]=c\n",
    "    words_iso = word_isolation(sentences,val,n)\n",
    "    \n",
    "    words_freq = word_freq(sentences,n)\n",
    "    words_dis={}\n",
    "    for i in xrange(1,n):\n",
    "        words_set=dict((key,value) for key, value in words_freq.iteritems() if len(key) == i)\n",
    "        for word in words_set:\n",
    "            if i<n:\n",
    "                base=pieces[i+1]\n",
    "                match1=[]\n",
    "                for b in base:\n",
    "                    if bool(re.search(\"^\"+word,b)):\n",
    "                        match1.append(b)\n",
    "                freq1=[]\n",
    "                for m in match1:\n",
    "                    freq1.append(words_freq[m])\n",
    "                SUM=sum(freq1)\n",
    "                log_a=[-(math.log(float(a)/SUM,2)) for a in freq1]\n",
    "                pre=numpy.mean(log_a)\n",
    "\n",
    "                match2=[]\n",
    "                for b in base:\n",
    "                    if bool(re.search(word+\"$\",b)):\n",
    "                        match2.append(b)\n",
    "                freq2=[]\n",
    "                for m in match2:\n",
    "                    freq2.append(words_freq[m])\n",
    "                SUM2=sum(freq2)\n",
    "                log_a2=[-(math.log(float(a)/SUM,2)) for a in freq2]\n",
    "                post=numpy.mean(log_a2)\n",
    "\n",
    "                if math.isnan(pre)|math.isnan(post):\n",
    "                    if math.isnan(pre):\n",
    "                        words_dis[word]=post\n",
    "                    else:\n",
    "                         words_dis[word]=pre\n",
    "                else:\n",
    "                    words_dis[word]=min([pre,post])    \n",
    "            else:\n",
    "                pass\n",
    "    return words_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test disorder\n",
    "import re\n",
    "import numpy\n",
    "import math\n",
    "val=10\n",
    "n=6\n",
    "sentences=text\n",
    "pieces={}\n",
    "for i in xrange(1,n+1):\n",
    "    c=[]\n",
    "    for sentence in sentences:\n",
    "        #c = chain.from_iterable([c,ngram(sentence,i)])\n",
    "        c=c+ngram(sentence,i)\n",
    "    pieces[i]=c\n",
    "    #for p in pieces[i]:\n",
    "    #    print p\n",
    "    #print \"----------\"\n",
    "words_iso = word_isolation(sentences,val,n)\n",
    "\n",
    "words_freq = word_freq(sentences,n)\n",
    "#for f in words_freq:\n",
    "#    print f\n",
    "words_dis={}\n",
    "print len(words_freq)\n",
    "i=0\n",
    "for word in words_freq:\n",
    "    i+=1\n",
    "    print \"no.\" + str(i)\n",
    "    try:\n",
    "        N=len(word)\n",
    "        if N<n:\n",
    "            base=pieces[N+1]\n",
    "            print \"字長度：\"+str(len(word))\n",
    "            print \"要比較的字:\" , word, \"^\"+word, word+\"$\"\n",
    "            print \"被比較的字:\"\n",
    "            match1=[]\n",
    "            match2=[]\n",
    "            for b in base:\n",
    "                print \"\\t\", b\n",
    "                print \"\\t\",\"  比較:\",\"^\"+word, bool(re.search(\"^\"+word,b))\n",
    "                print \"\\t\",\"  比較:\",\"^\"+word, bool(re.search(word+\"$\",b))\n",
    "                #print \"-------\"\n",
    "                if bool(re.search(\"^\"+word,b)):\n",
    "                    match1.append(b)\n",
    "                if bool(re.search(word+\"$\",b)):\n",
    "                    match2.append(b)\n",
    "            '''print \"match-pre: \"\n",
    "            for m in match1:\n",
    "                print m\n",
    "            print \"match-post: \"\n",
    "            for m in match2:\n",
    "                print m'''\n",
    "                \n",
    "            \n",
    "            freq1=[]\n",
    "            for m in set(match1):\n",
    "                print \"match-pre:\",m,words_freq[m]\n",
    "                freq1.append(words_freq[m])\n",
    "            print \"freq-pre:\",freq1\n",
    "            print \"match1\", match1\n",
    "            \n",
    "            SUM=sum(freq1)\n",
    "            print \"SUM-pre:\",SUM\n",
    "            log_a=[-(math.log(float(a)/SUM,2)) for a in freq1]\n",
    "            print \"log-pre:\",log_a\n",
    "            pre=numpy.mean(log_a)\n",
    "            print \"mean-pre:\",pre\n",
    "\n",
    "            freq2=[]\n",
    "            for m in set(match2):\n",
    "                print \"match-post:\",m,words_freq[m]\n",
    "                freq2.append(words_freq[m])\n",
    "            print \"freq-post:\",freq2\n",
    "\n",
    "            SUM2=sum(freq2)\n",
    "            print \"SUM-post:\",SUM2\n",
    "            log_a2=[-(math.log(float(a)/SUM,2)) for a in freq2]\n",
    "            print \"log-post:\",log_a2\n",
    "            post=numpy.mean(log_a2)\n",
    "            print \"mean-post:\", post\n",
    "            #mean(-log2(freq1/sum(freq1)))\n",
    "\n",
    "            if math.isnan(pre)|math.isnan(post):\n",
    "                if math.isnan(pre):\n",
    "                    words_dis[word]=post\n",
    "                    print \"亂度:\", words_dis[word]\n",
    "                else:\n",
    "                    words_dis[word]=pre\n",
    "                    print \"亂度:\", words_dis[word]         \n",
    "            else:\n",
    "                words_dis[word]=min([pre,post]) \n",
    "                print \"亂度:\", words_dis[word]\n",
    "        else:\n",
    "            print '字長度:',len(word)\n",
    "            pass\n",
    "        print \"========\"\n",
    "    except:\n",
    "        print \"error\"\n",
    "        print \"========\"\n",
    "        pass\n",
    "print words_dis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
